Every natural language processing tool has
to be evaluated and language models have
to be evaluated as well. What does it mean
for Tore Language Model to be a good
language model? In general, we say a good
language model is one that is better at
finding the good sentences and predicting,
and liking them more than the bad
sentences. Or, more specifically [sound],
we want to assign a higher probability to
real or perhaps frequently observed
sentences than ungrammatical or impossible
or at least rarely observed sentences. So,
that's the goal of our evaluating a
language model. Now we train the
parameters of a language model on a
training set, and we test the model's
performance on data that we haven't seen.
So we have some training data and some
unseen data. This unseen data is called a
test set. And we want it to be something
that's not the same as our training set,
totally unused, we've never looked at it
before. And that will be a fair evaluation
of our model. And then we'll need an
evaluation metric that tells you how well
does your model do on this unseen test
set. So what are these evaluation models?
The best evaluation, best way of comparing
any two models, two language models, A and
B, is to put each model in a task. So
we're gonna build our spelling corrector,
or speech recognizer or MT system.
Whatever our application is that uses
language models. We'll put our language
model in there and now we'll run our
task, and we'll get some accuracy for the
system running with model A, the system
running with model B. Perhaps that's how
many misspelled words are corrected
properly if we're doing spelling
correction. Or how many words are
translated correctly if we're doing
translation. And now we just compare the
accuracy of the two models, whichever
model has the higher accuracy is the,
better language model. So this is called
extrinsic evaluation, were using something
external to the N-gram model itself and
looking at our performance on that
external task. The problem with this kind
of extrinsic evaluation, it's also called
invivo evaluation. And is but it's time
consuming in many cases this could take
days or weeks for a modern machine
translation system or modern speech
recognition system running evaluations can
often be extremely slow. So instead, what
we sometimes use is an intrinsic
evaluation. Something that's about,
intrinsically about language models
themselves and not about any particular
application. And the most common intrinsic
evaluation is called, perplexity. Now
perplexity happens to be a bad
approximation to an extrinsic
evaluation unless it turns out that the
test data looks a lot like the training
data. So generally perplexity is useful
only in pilot experiments, but it does
help to think about the problem and it's a
useful tool as long as we also use
extrinsic evaluation as well. So let's
think about the intuition of perplexity.
And like many ideas in language modeling
this dates back to Claude Shannon. So, so
Shannon proposed among many other things a
game about word prediction. How well can
we predict the next word. So, for example,
we've seen sentences like, I always order
pizza with cheese and. And our job is to
predict the next word. So from this first
sentence, we might say, well, a good
language model might guess that we are
likely to have mushrooms and likely to
have pepperoni and maybe less likely to
have anchovies because anchovies are
somewhat less popular than mushrooms. And
very unlikely to put fried rice on our
pizza. And extremely unlikely, let's say,
to have and, and. Although people, I
guess, do say and, and. After, after the
word and. And so the how well the model
predicts the, the actual words that occur
is, is the intui-, is, is how good the
model is. So a model. On a sentence, on a
sentence like the thirty third president
of the US we know the next word is very
likely to be JFK or John or Kennedy or
some word like that. So this is a very
predictable case. Here we have, I saw
anything could come next. So, in some
cases we're gonna do, be, be much better
predicting the next word, in some cases
very much worse. But a good language model
on average should do better than a bad
language model. Now it turns out that
unigrams are very bad at this game, and if
you think for a second you'll realize why.
So in summary, a better model of text, a
better language model, is one that assigns
a higher probability, assigns a higher
probability to whatever word actually
occurs. If you can guess right, the next
word, you are a good language model. So
the best language model is one that best
predicts an unseen test set, or assigns,
on average, the highest probability of a
sentence to all the sentences that it
sees. If I've seen this, if I see this
test set and I assign, give me a new test
set, and I assign a probability to each of
those sentences, the better language model
is the one that says, oh, I knew that
sentence was coming and assigns it a very
high probability. Now the perplexity, this
new metric we're going to be using, is the
probability of the test set normalized by
the number of words. So we'll take, let's
say our test set is, is a long sentence,
N, N words long. So we'll take this N word
sentence, we'll take its probability, and
we'll take the one over [inaudible], that,
we'll take the [inaudible] rou-, the
[inaudible] route. So, and we'll take the
inverse of it. So, it's, it's a way of
normalizing for the length of the
probability. So, it's, it's, take this
long sentence, take the, probably the
whole sentence, and normalize by the
number of words, because obviously long
sentences, the longer the sentence, the
less probable it's going to be. So, we
want some normalizing factor so we can
compare test sets of different lengths. So
that's the enthru of one over, the
perplexity of a string of words, W, is the
enthru of one over the probability of the
string of words. So, this parentheses
should be here. So, by the chain rule.
That's the, the probability of this string
of words one through N is the probability
overall I sorry I'm sorry the product
overall I of the probability of each word
given the entire prefix before hand. And
so we've just, by the chain rule, replaced
the, probability of a long sequence with
the product of the probabilities of each
word, given its prefix. And then, for
bigrams, by our mark of approximation to
the chain rule, we can say that the,
probability, we've replaced the
probability of a sequence of words with
the product of a bunch of bigrams. So the
perplexity of a string of words is the
[inaudible] route of the product of, of,
N, by gram probabilities multiplied
together and inverted, so it's just a
function of the probability of the
sentence. So because of this inversion
minimizing perplexity is the same as
maximizing probability. There is another
intuition for perplexity also based on
Shannon and, and this example comes from
Josh Goodman and this [inaudible], the
second intuition for perplexity relies on
the idea that perplexity is the average,
related to the average branching factor.
Perplexity at any point in a, in a
sentence is on average, how many things
can occur next? And we'll see later this
is related to the probability of the
upcoming things, related to the entropy of
the upcoming things. But roughly speaking
if I had ten possible word that can come
next and they were all equal probability
my perplexity will be ten. So for example
if I'm recognizing the ten digits. Then
the perplexity of the task is ten.
There's ten possible things that could
come next and I can't decide between them.
If I have to repre-, to recognize am I
building a speech recognizer for, for a,
a, a, switchboard phone service and I have
to recognize 30,000 names. Then the
perplexity of the names is 30,000 if
they're all equally likely. But suppose a
system has to represent, has to recognize,
let's say again a phone, switchboard phone
operator, automatic phone operator. Has to
recognize the word operator and that
occurs a one-fourth of the time. The word
sales that occurs a quarter of the time or
the word technical support, that occurs a
one-fourth of the time. And then with one
over 120,000 times each, another 30,000
names occur. So now we have to take the
weighted average of all these
possibilities of what could occur, to
compute, on average, how likely is any one
word to occur and now the perplexity is
54. So the perplexity again is the
weighted equivalent branching factor. So
lets examine this new kind of perplexity,
the weighted equivalent branching
factor, and show that it?s the as this
inverted normalize probability metric. So
let?s take a sentence containing random
digits, what?s the perplexity of this
sentence according to a model, that assign
equal probability to each digit? So we'll
see perplexity of this sentence, this
string of digits lets make it, lets make
it. [inaudible], it doesn't matter how
long it is. So we have a bunch of digits.
And the probability of this bunch of
digits, we'll call them digit one, digit
two, through digit N. The perplexity by
our first metric is negative one. Is, is
the probability of this sequence to the
negative one over N. And since we've said
that each of these words has probability
1/10, one-tenth. And we're assuming a
unigram probability. So that's the
probability of one-tenth x one-tenth x
one-tenth x one-tenth, and so on. So
that's the prob-, that's one-tenth. To the
n. Because there's, there were N words to
the negative one over N. And as that we
can see over here that's equal to the N's
canceled we get one-tenth of the minus one
or we get ten so by thinking about
perplexity as the normalized probability
of a long string. We can sort of see the
intuition that the average branching
factor by normalizing for the length.
We're sort of asking how many things can
occur each time waited by their
probability. Alright. Now, so perplexity
in general the lower the perplexity the
better of the model. So, for example
here's, here's where a training set
trained on 38 million words tested on, on
1.5 million words from the newspaper The
Wall Street Journal and a unigram model
has a perplexity of 962. A bi-gram model
has a much lower, much more accurate
perplexity of 170 and tri-gram model has
an even lower per, perplexity. So,
perplexity since it's modeling something
like average branching factor or average
predictability the lower you get. The
better you are at predicting the, the
model, the actual data that occurs.
