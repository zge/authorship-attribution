How do we estimate these N-gram
probabilities? Let's look at bigram
probabilities. The maximum likelihood
estimate for a bigram probability, the
probability of a word i, given the
previous word i-1, we just estimate by
counting. We count, how many times did
word i-1 and i occur together? And divide
it by how many times word i-1 occurs. So
it's like saying, of all the times that we
saw word i-1, how many times was it
followed by word I? And we'll use the
notation count sometimes. And sometimes,
we'll, for simplification, we'll just
refer to a c, we'll use c for count? So,
the joint count of word i-1 and i, divided
by the count of word i-1. So let's walk
through an example. So we have again our
equation, the probability of word i given
the word i-1 is the maximum like
the estimate is just. We'll write, I'll
write MLE. Estimated by the maximum
likelihood estimator as, as this count
over this count. And let's look at our,
let's look at, let's make up a corpus. So,
here's our simple corpus borrowed by Dr. Seuss.
So, we have three sentences, each
one starts with a special token start up
sentence, and end with a special token end
of sentence. And they're very short
sentences. "I am Sam", "Sam I am", "I do not
like green eggs and ham". Let's compute
some language model probabilities from
this small corpus. So, first probability
of i given the start symbol, so that's
computed as count of start symbol comma 'I'
- with a big I - over count of start
symbol. So 'I' follows the start symbol twice:
one, two, and the start
symbol occurs three times: one, two,
three. So that probability is two-thirds or .67. So that's the probability of 'I' given
start. And you can see common examples of
lots of different probabilities here. So
for example, let's just pick another one
at random. The probability of 'Sam' given
the word am, how many times does ('am', 'Sam')
occur? It occurs once. So that's one.
And the denominator is, how many
times does 'am' occur? And that occurs
twice. So that's gonna be one over two. So
here's our probability of 'Sam' given 'am', and
so on. So let's look at a larger corpus in
order to get some more realistic counts.
And the corpus we're using here was
collected from a dialog system that
answered questions about restaurants in
the city of Berkeley, California. And
here's the kind of sentences that were in
this corpus: "Can you tell me about any
good Cantonese restaurants close by?" Or,
"Mid-price Thai food is what I'm looking
for.", "Tell me about Chez Panisse." Now I
hear some sentences, and let's
compute some N-grams based on these
sentences. So first, let's start with the
raw bigram counts from the small corpus
of just under 10,000 sentences. So, what
I'm showing you here is a bigram count
table, so here the word 'I' is followed
by the word 'I' five times. The word 'I' is
followed by the word 'want' 827 times, the
word 'want' is followed by the word 'to' 680
times, the word 'to' is followed by the word
'eat' 686 times and we've just put some
sample word up here, I picked "I, want, to, eat,
Chinese, food, lunch, spend", just to
show you some words that might occur
together in a sentence and some other word
and you can see a lot of these words, a
lot of these probabilities are zero, a lot
of these counts, I'm sorry are zero,
because just happened in this small data
set that 'want' was never followed by 'want'.
So that's this zero here or 'Chinese' was
never followed by the word 'to' here. Okay.
So in order to turn these counts into
probabilities all we have to do is
normalize by a unigram count. Because
remember the probability of a word I given
the word i minus one. Is the count of word
i-1. Word i over the count of word i-1.
So, we need to divide these joint
counts of the two words, by the count of
the previous word. Here's the uni-gram
counts we're going to need to compute
those probabilities. So, here's the count
if 'I', it's 25/33. Here's the count of 'eat',
it's 7/46. And using the equation we can
now compute the bi-gram probability. The
probability for example of "to" given "want".
How likely is it, given that the previous word
was "want" that the next word is "to". And
it's pretty likely. For example .66. But
notice that things with counts of zero
still have probabilities of zero. Lots of
things have zeros here. So now, that we've
computed all of our bigram capabilities,
we can now estimate the probability of the
sentence, that's our goal for language
modeling. Simply by multiplying together
all the component probabilities. So the
probability of " I want
English food " is
probability of 'I' given 'start', times the
probability of 'want' given 'I', times the
probability of 'English' given 'want', 'food'
given 'English', 'start' [end of sentence ] given 'food' and so on.
What kinds of knowledge are expressed by
these bigram probabilities. Why, for
example is the probability of 'English'
given 'want' lower than the probability of
'Chinese' given 'want'. Well, probably that's
because Chinese food is more popular and
more people are gonna ask about it and so
wanting Chinese is more likely than wanting English,
and  that's a fact about the
world. It's a fact about cuisines, not so
much a fact about English. What about the
probability of 'to' given 'want' being so high?
Now that's a fact about grammar.
That's a fact that the verb 'want', in
English, requires an infinitive after it.
So 'want' has infinitives, and that's a
grammatical fact. So that's
grammar, we'll put grammar. And what about
the probability that 'want', given the
previous word is 'spend', is zero? So that
zero seems to be caused by a grammatical
fact: 'spend', 'want' is two verbs in a row,
that kind of verb doesn't seem to be
grammatically possible in English. So that
zero is caused by a grammatical
disallowing. How about this zero? What's
the, why is the probability of 'food'
following 'to' is zero? Now here, this zero is a
contingent zero. This, you could imagine a
sentence that has 'to food' in it. I'd like
you to stop and think about a sentence
like that yourself. Good. It just happens
that such a sentence never occurred in the
training data. So this is a contingency
row, where this is a structural zero. All
right, let's move on. In practice, we don't
keep these probabilities in the form
of probabilities, in fact we keep them in
the form of log probabilities and there are two reasons for
this, one is, we is can avoid underflow,
if you think about it: If you have a very
long sentence and you're multiplying together
20 or 30 or 40, little to tiny
probabilities - each of which is a very small number less then zero -
when you multiply with so many
small numbers, you get a very small number
that often ends up with arithmetic
underflow, in the computation. And so, we
want to avoid this kind of underflow. And
it turns out that adding is faster than
multiplying anyway. So, again, instead of
multiplying four probabilities we'll, in
general, just add four log probabilities.
Then we're going to store our language
models in logs. There are a number of
publicly available language modeling toolkits.
One of them, SRILM - and you can download SRILM.
Another publicly available resource is the Google N-Grams
release, this has been out for over
five years now. Google released a trillion
word corpus. Over a trillion five grams
and thirteen million unique words. So a
huge data set which, you can download and
use for your, any kind of N-gram
applications you'd like to use. So here's
an example, some of the data from the
Google N-Grams release. Some four-gram
counts for four-grams beginning with "serve
as the" and words beginning with 'in'. So
this is a very big corpus, and, and you
can see that "serve as the indication"
occurred 72 times on this web corpus. And
you have more information on the, Google
web corpus there. Another publicly
available corpus is the Google book N-gram
corpus. Let's look at that. So this corpus
lets you plot counts of words in Google
books. And a number of corpora are
available for American English, British
English, Chinese, French, and German, and,
various kinds of corpora, which can all be downloaded.
