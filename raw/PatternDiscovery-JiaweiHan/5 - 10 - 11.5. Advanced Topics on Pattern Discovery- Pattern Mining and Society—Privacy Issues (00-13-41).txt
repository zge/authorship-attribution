[SOUND] In this session, we're going
to discuss another important issue.
&nbsp;Pattern Mining and Society.
Especially, we will be focusing on
the privacy issue in Data Mining.
We know, we are living in a data rich or
big data society.
This of course create, you know, create
opportunity for a successful data mining.
But on the other hand,
people do have concerns whether
pattern mining may violate
people's privacy or security.
Okay.
Actually, data mining may
naturally have potential
adverse side effect is
compromise the privacy.
The privacy and accuracy are typically
contradictory in nature.
That means, if you want your
pattern mining very accurate.
In many cases,
the privacy may be compromised.
But if you want to protect your
privacy in a very tight manner,
then the accuracy of
the pattern mining may suffer.
Okay.
So
the problem is we need a new technology
to privacy preserving data mining.
We ensure the data mining
will achieve good results.
But in the mean time,
we're going to protect people's privacy.
There are lots of research
working on this issue.
We call privacy preserving data mining.
Okay.
The study generally focused on
three categories on privacy.
One, we call input privacy or
you can think about data privacy.
Essentially, how do we hide data?
We can distort data,
we can hide data to prevent data miners
from reliable extracting confidential or
private information.
Another category of privacy
is output privacy or we say,
knowledge privacy,
knowledge hiding problem.
That means, where the input data may
be okay, but once we find patterns,
certain pattern or
knowledge could be sensitive.
You may not want to disclose it.
So how we can guarantee?
We do the mining,
we can ensure the output privacy.
Okay.
This is another interesting issue.
The third issue people
studied called owner privacy.
That means the people,
any party may host certain data.
For example, different hospitals may hold
different kinds of patient information.
You may want to study, for example,
AIDS or heart problems or lung cancer or
those problems.
Okay.
But you may not want to see
where the data come from.
That means, you actually want
to hide the data owners or
the hospitals hide the source of the data,
then you can do more, you know, study.
This we call owner privacy problem.
So, in this short session,
we are going to focus on how to
ensure input privacy or
you can say, data privacy problem.
Okay.
Of course, one simple approach could
be you trust service provider.
They are going to anonymize their data
especially the use of private information.
They're going to anonymize them and
they are going to ship those
anonymized data to data miners.
This model we call
business-to-business environment or
service provider-to-data
miner environment.
That means you have to trust both parties.
But the problems,
do you really trust them?
And most people may say, no.
Okay.
So that's the one, of course,
this could be quite popular in, in
the regular practice, but you want more.
Okay.
So the second approach we call data
anonymization or perturbation.
That means, we ensure data privacy and
owners cited data source site.
Usually, this anonymization even you
may not trust the service providers.
You actually,
will try to ask a third party vendor
to anonymize the data before
they can release to anybody.
Okay.
This one we call business-to-customer
environment.
And the typical method develop,
like a data perturbation,
data transformation, data hiding.
That means, we know that hides some
sensitive value or sensitive attributes.
Let's look at this table, this is a,
suppose we have a hospital,
it contains certain problem of,
like the data is patient ID,
you get zip code, you get age,
you get the disease.
People may not be comfortable
on this kind of data,
because even you anonymize patient name,
but you may got to,
some people may know the patient's age,
and zip code.
They can easily say, oh,
this person got a heart problem.
Okay.
So then the message could be, what about
we, we make it a little more generalized.
We hide the last two digit,
digits of zip code,
then we hide the last digit of their age.
That means you only know is add a zero,
one, eight, something,
zip code, and in the 40s,
got a heart problem.
But then you see for the same,
you know zip code and
same 40s, there are lots of other disease.
So you will have no way to tell this
person whether they got a heart problem,
cancer, flu or anything else.
So in that sense,
you may feel more comfortable.
If you got three records, you may
still not feel very comfortable, but
what about you get 100?
You probably say that's okay.
So this actually records
k-anonymity problem.
That means every equivalent class,
that means they have the same zip code and
h, they at least have case records.
If this k is really large, you'll
feel probably much more comfortable.
Because within this 100 different disease,
nobody can guess what you have.
Okay.
So, but this may not still be sufficient.
For example,
what about all these age, the,
the zip code and in the thirties,
everybody got a diabetes.
Then, you know, then you agree to use it,
then you can see you do get diabetes.
Okay.
So that means,
this record even this k is not sufficient,
you want ensure this disease
is sensitive attribute got at
least the same values, so
we call this l diversity.
But even this l diversity,
what about the majority of
people who have diabetes you still
feel not the, not comfortable.
Okay.
So there are other studies,
further studies for example t-closeness,
differentially privacy.
So there are lots of research
I'm doing to see how to
insure people have no way
to guess what you have.
Of course even for error diversity,
if the majority supposed is HIV negative,
probably this, this majority
will not really have concern,
because, you know,
HIV negative is not a concern.
Okay.
So that's the thing the approach can see
there are, there are many issues to
study to ensure privacy in this way.
Okay.
So, I'll not get into detail, but
there are many good research papers I list
at the end some typical papers,
you may like to go deeper by reading them.
Now, I'm going to discuss a little
data perturbation methods for
Privacy-Preserving Pattern Mining.
Data peturb, perturbation usually
people use statistic approach.
That means using some randomization
algorithms to distort the data.
One way you can do is we call
independent attribute perturbation.
That means, you take each attribute.
You perturb the, the values
independent of the other attributes.
Okay.
You don't have to think about
their other attributes existing.
Like you perturb the zip code or
you perturbed the h.
So, it doesn't matter.
Okay.
But for pattern mining,
you will find the correlations.
So sometimes, you may like to see
dependent attribute perturbation, because,
because the purer independent you do
perturb these correlation may be lost.
That means, if you want to take care
of the correlation across attributes,
your may concern to develop some
dependent attribute perturbation method.
Okay.
And for this data perturbation,
there are some interesting studies.
One method called MASK.
The, that method was if you
think your shopping transaction
contain lots of items.
Okay.
Usually, one transaction may only,
that means one shopping basket may
only contain ten or fifteen items.
But if you go to Walmart,
there are so many items,
you can think all these
items is a very long vector.
Okay.
In your own shopping basket, only small
number of these factor turns to one.
Okay.
But you want to hide what exactly you,
you, you are buying.
Okay.
So, in that sense,
you may turn the other
items from zero to one and
some of your one items will turn from
one to zero with certain probability.
Okay.
With this,
if you're carefully tuning this p,
you may achieve both good average privacy,
because you insert a lot of randomly,
particular items.
They were will not figure
out what you bought, but
you still will get a good accuracy.
The reason is in general those pattern,
once you randomize a lot of different
items they, with this randomization and
likely, they will become frequent.
Okay.
So the pattern may real
frequent pattern mix you up.
But this method actually may turn, because
of the majority of things are zeros.
You may turn many things into
one with certain probability, so
that may increase your number of items
in each transaction quite a lot,
make your database really big.
Okay.
Either you can find patterns it could be
substantially slow.
There are stu,
studies now how to, you know,
ensure both, you know,
efficiency and accuracy.
Okay.
Another method studied called cut and
paste operator.
This essentially is using
uniform randomization.
The method is you get a real transaction,
you don't increase items
in this real transaction.
But you take the existing item,
you have a probability to
randomly replace some existing
item with a new item.
Okay.
Not presenting your original transaction.
So as long as you do not make your
probability too big to replace them,
you still get a good patterns show up.
So but sometimes, if you replace too much,
you go quite worst case.
You know, on the accuracy or
you will pay is too little,
you may get worst case privacy.
So their masters studied
how to balance both.
How to select items in an organized way.
Okay.
To improve the worst case privacy,
but still keep good accuracy.
Actually, the experiments show,
if you want to mine, you know,
short transactions,
short patterns, short itemsets.
Okay.
So these cut and
paste randomize the database you mine it.
You may correctly identifies
80 to 90% short patterns,
like lens, less or equal to three.
But if you want to mine long patterns,
because such perturbation,
a lot of the long pattern
may be destroyed.
Okay.
So how to effectively mine such long
patterns, still remain good privacy.
Its an open problem.
Okay.
So privacy preserving data mining is
still a very active research field.
There are lots of research
papers published.
For example, R Agrawal,
they got the first paper published
in year 2000 in Sigmod and
there are books.
And they, I,
also intentionally put a privacy,
differential privacy like t-closeness,
l-diversity,
those papers here, because I did not
really lecture this to any detail.
Okay.
So interested reader,
you may like to read those if you want,
you are interested in, you know,
further push forward the frontier of
privacy preserving and data mining.
Thank you.
[MUSIC]

