In this video, I will introduce Restricted
Boltzmann Machines.
These have a much simplified architecture
in which there are no connections between
hidden units.
This makes it very easy to get the
equilibrium distribution of the hidden
units if the visible units are given.
That is, once you've clamped the
datavector on the visible units,
The equilibrium distribution of the hidden
units can be computed exactly in one step
because they're all independent of one
another, given the states of the visible
units.
The proper Boltzmann machine learning
algorithm is still slow for a restricted
Boltzmann machine.
But in 1998, I discovered a very
surprising shortcut that leads to the
first efficient learning algorithm for
Boltzmann machines.
Even though this algorithm has theoretical
problems, it works quite well in practice.
And it led to a revival of interest in
Boltzmann machine learning.
In a restricted Boltzmann machine, we
restrict the connectivity of the network
in order to make both inference and
learning easier.
So, it only has one layer of hidden units
and there's no connections between the
hidden units.
There's also no connections between the
visible units.
So, the architecture looks like that, it's
what computer scientists call a bipartite
graph.
There's two pieces, and within each piece,
there's no connections.
The good thing about an RBM is that if you
clamp a datavector in the visible units,
you can reach thermal equilibrium in one
step.
That means with a datavector clamped, we
can quickly compute the expected value of
vihj because we can compute the exact
probability with each j will turn on, and
that is independent of all the other units
in the hidden layer.
The probability that j will turn on is
just the logistic function of the input
that it gets from the visible units and
quite independent of what other hidden
units are doing.
So, we can compute that probability all in
parallel and that's a tremendous win.
If you want to make a good model of a set
of binary vectors, then the right
algorithm to use for a restricted
Boltzmann machine is one introduced by
Tieleman in 2008 that's based on earlier
work by Neal.
In the positive phase, you clamp the
datavector on the visible units.
You then compute the exact value of the
expectation vihj for all pairs of
invisible in the hidden unit.
And you could do that cuz vi is fixed, and
you can compute vj exactly.
And then, for every connected pair of
units, you average the expected value of
vihj over all the data vectors in the mini
batch.
For the negative phase, you keep a set of
fantasy particles that is global
configurations.
And then, you update each fantasy particle
a few times by using alternating parallel
updates.
So, after each weight update, you update
the fantasy particles a little bit and
that should bring them back to close to
equilibrium.
And then, for every connected pair of
units, you average vihj over all the
fantasy particles, and that gives you your
negative statistics.
This algorithm actually works very well,
and allows RBMs to build good density
models or sets of binary vectors.
Now, I am going to go on to our learning
algorithm that is not as good at building
density model but is much faster.
So, I'm going to start with a picture of
an inefficient learning algorithm for
restrictive Boltzmann machines.
We're going to start by clamping a
datavector on the visible units, and we're
going to call that time t0.
So, we're going to use times now, not to
denote weight updates, but to denote steps
in a Markov chain.
Given that visible vector, we now update
the hidden units.
So, we choose binary states for the hidden
units and we measure the expected value,
vihj, for all pairs of visible and binary
units that are connected.
And I'll call that vihj zero to indicate
that it's measured at time zero,
With the hidden units being determined by
the visible units.
And, of course, we can update all the
hidden units in parallel.
We then use the hidden vector to update
all the visible units in parallel, and
again we update all the hidden units in
parallel.
So, the visible vector t1 = one, we'll
call a reconstruction, or a one-step
reconstruction,
And we can keep going with the alternating
chain that way,
Updating visible units, and then hidden
units,
Each set being updated in parallel.
And after we've gone for a long time,
We'll get to some state of the visible
units, or I'll call t infinity to indicate
it needs to be a long time and the system
will be at thermal equilibrium, and now,
we can measure the correlation of vi and
hj after the chains run for a long time
and I'll call that vihj infinity.
And the visible state we have after a long
time, I'll call it fantasy.
So now, the learning rule is simply, we
change Wij by the learning rate times the
difference between vihj at time zero and
vihj at time infinity.
And, of course, the problem with this
algorithm is that we have to run this
chain for a long time before it reaches
thermal equilibrium.
And if we don't run it for long enough,
the learning may go wrong.
In fact, that last statement is very
misleading.
It turns out that even if we only run the
chain for a short time, the learning still
works.
So, here's the very surprising shortcut.
You just run the chain up, down, and up
again.
So, from the data, you generate a hidden
state, from that.
You generate a reconstruction, and from
that, you generate another hidden state.
And you may have a statistics once you've
done that.
So, instead of using the statistics
measured at equilibrium, we're using the
statistics measured after doing one full
update of the Markov chain.
The learning rule is, and the same as
before, except this much quicker to
compute, and this is clearly is not doing
maximum likelihood learning because the
term we are using for negative statistics
is wrong.
But the learning, nevertheless, works
quite well.
Next week, we'll understand a bit more
about why it works well.
But for now, we'll just see that it does.
So, the obvious question is why does
actual cut work at all?
And here's the reasoning.
If we start the chain at the data, the
Markov chain will wander away from the
data and towards its equilibrium
distribution.
That is towards things that is initial
weights like more than the data.
We can see what direction it's wandering
in after only a few steps.
And if we know the initial weights aren't
very good, it's a waste of time to go all
the way to equilibrium.
We know how to change them to stop it
wandering away from the data without going
all the way to equilibrium.
All we need to do is lower the probability
of the reconstructions of confabulations
as a psychologist would call them, it
produces after one full step, and then,
raise the probability of the data.
That will stop it wandering away from the
data.
Once the data and the places it goes to
after one full step have the same
distribution, then the learning will stop.
So, here's a picture of what's going on.
Here's the energy surface in the space of
global configurations.
Here's a data point on the energy surface,
and by data point, I mean, both the
visible vector and the particular hidden
vector that we got by stochastic updating
the hidden units.
So, that hidden vector is a function of
what the data point is.
So, starting at that data point, we run
the Markov chain for one full step to get
a new visible vector and the hidden vector
that goes with it.
So, a reconstruction of the data point
plus the hidden vector that goes with that
reconstruction.
We then change the weights to pull the
energy down at the data point, and pull to
the energy up the reconstruction.
And the effect of that would be to make
the surface look like this.
And you'll notice we're beginning to
construct an energy minimum at the data.
You'll also notice that far away from the
data, things have stayed pretty much as
they were before.
So, this shortcut of only doing one full
step to get the reconstruction fails for
places that are far away from the data.
We need to worry about regions of the
data-space that the model likes but which
are very far from any data point.
These low energy holes cause the
normalization term to be big, and we can't
sense them if we use the shortcut.
If we use persistent particles, where we
remembered their states, and after each
update, we updated them a few more times,
then they would eventually find these
holes.
They'd move into the holes, and the
learning would cause the holes to fill up.
A good compromise between speed and
correctness is to start with small weights
and to use CD1, that is contrust
divergence with one full step to get the
negative data.
Once the weights have grown a bit, the
Markov chain is mixing more slowly, and
now we can use CD3.
Once the weights have grown more, we can
use CD5, or nine, or ten.
So, by increasing the number of steps as
the weights grow, we can keep the learning
working reasonably well, even though the
mixing rate of the Markov chain is going
