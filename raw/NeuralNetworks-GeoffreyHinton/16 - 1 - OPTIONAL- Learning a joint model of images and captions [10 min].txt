In this video, I'm going to talk about 
some recent work on learning a joint 
model of captions and feature vectors 
that describe images. 
In the previous lecture, I talked about 
how we might extract semantically 
meaningful features from images. 
But we were doing that with no help from 
the captions. 
Obviously the words in a caption ought to 
be helpful in extracting appropriate 
semantic categories from images. 
And similarly, the images ought to be 
helpful in disambiguating what the words 
in the caption mean. 
So the idea is we're going to try in a 
great big net that gets its input, stand 
to computer vision feature vectors 
extractive for images and pack up words 
representations of captions and learns 
how the two input representations are 
related to each other. 
At the end of the video I'll show you a 
movie of the final network using words to 
create feature vectors for images and 
then showing you the closest image in its 
data base. 
And also using images to create bytes of 
words. 
I'm now going to describe some work by 
Natish Rivastiva, who's one of the TAs 
for this course, and Roslyn Salakutinov, 
that will appear shortly. 
The goal is to build a joint density 
model of captions and of images except 
that the images represented by the 
features standardly used in computeration 
rather than by the ropic cells.This needs 
a lot more computation than building a 
joint density model of labels and digit 
images which we saw earlier in the 
course. 
So what they did was they first trained a 
multi-layer model of images alone. 
That is it's really a multi-layer model 
of the features they extracted from 
images using the standard computer vision 
features. 
Then separately, they train a multi-layer 
model of the word count vectors from the 
captions. 
Once they trained both of those models, 
they had a new top layer, that's 
connected to the top layers of both of 
the individual models. 
After that, they use further joint 
training of the whole system so that each 
modality can improve the earlier layers 
of the other modality. 
Instead of using a deep belief net, which 
is what you might expect, they used a 
deep Bolton machine, where the symmetric 
connections bring in all pairs of layers. 
The further joint training of the whole 
deep Boltzmann machine is then what 
allows each modality to change the 
feature detectors in the early layers of 
the other modality. 
That's the reason they used a deep 
Boltzmann machine. 
They could've also used a deep belief 
net, and done generative fine tuning with 
contrastive wake sleep. 
But the fine tuning algorithm for deep 
Boltzmann machines may well work better. 
This leaves the question of how they 
pretrained the hidden layers of a deep 
Boltzmann machine. 
because what we've seen so far in the 
course is that if you train a stack of 
restricted Boltzmann machines and you 
combine them together into a single 
composite model what you get is a deep 
belief net not a deep Boltzmann machine. 
So I'm now going to explain how, despite 
what I said earlier in the course, you 
can actually pre-trail a stack of 
restrictive Boltzmann machines in such a 
way that you can then combine them to 
make a deep Boltzmann machine. 
The trick is that the top and the bottom 
restrictive bowser machines in the stack 
have to trained with weights that it 
twices begin one directions the other. 
So, the bottom Boltzmann machine, that 
looks at the visible units is trained 
with the bottom up weights being twice as 
big as the top down weights. 
Apart from that, the weights are 
symmetrical. 
So, this is what I call scale 
symmetrical. 
But the bottom up weights are always 
twice as big as their top down 
counterparts. 
This can be justified, and I'll show you 
the justification in a little while. 
The next restrictive Boltzmann machine in 
the stack, is trained with symmetrical 
weights. 
I've called them two W, here rather then 
W for reasons you'll see later. 
We can keep training with restrictive 
bowsler machines like that with genuinely 
symmetrical weights. 
But then the top one in the stack has 
be-trained with the bottom up weights 
being half of the top down weights. 
So again, these are scale symmetric 
weights, but now, the top down weights 
are twice as big as the bottom up 
weights. 
That's the opposite of what we had when 
we trained the first restricted Bolton 
machine in the stack. 
After having trained these three 
restricted Bolton machines, we can then 
combine them to make a composite model, 
and the composite model looks like this. 
For the restricted Bolton machine in the 
middle, we simply halved its weights. 
That's why they were 2W2 to begin with. 
For the one at the bottom, we've halved 
the up-going weights but kept the 
down-going weights the same. 
And for the one at the top we've halved 
the down-going weights and kept the 
up-going weights the same. 
Now the question is: Why do we do this 
funny business of halving the whites? 
The explanation is quite complicated but 
I'll give you a rough idea of what's 
going on. 
If you look at the layer H1. 
We have two different ways of inferring 
the states of the units in h1, in the 
stack of restricted bolts and machines on 
the left. 
We can either infer the states of H1 
bottom up from V or we can infer the 
states of H1 top down from H2. 
When we combine these Boltzmann machines 
together, what we're going to do is we're 
going to an average of those two ways of 
inferring H1. 
And to take a geometric average, what we 
need to do, is halve the weights. 
So we're going to use half of what the 
bottom up model says. 
So that's half of 2W1. 
And we're going to use half of what the 
top down model says. 
That's half of 2W2. 
And if you look at the deep Boltzmann 
machine on the right, that's exactly 
what's being used to infer the state of 
H1. 
In other words, if you're given the 
states in H2, and you're given the states 
in V, those are the weights you'll use 
for inferring the states of H1. 
The reason we need to halve the weights 
is so that we don't double count. 
You see, in the Boltzmann machine on the 
right. 
The state of H2 already depends on V. 
At least it does after we've done some 
settling down in the Boltzmann Machine. 
So if we were to use the bottom up input 
coming from the first restricted 
Boltzmann Machine in the stack. 
And we use the top down input coming from 
the second Boltzmann Machine in the 
stack, we'd be counting the evidence 
twice.'Cause we'd be inferring H1 from V. 
And we'd also be inferring it from H2, 
which, itself, depends on V. 
In order not to double count the 
evidence, we have to halve the weights. 
That's a very high level and perhaps not 
totally clear description of why we have 
to half the weights. 
If you want to know the mathematical 
details, you can go and read the paper. 
But that's what's going on. 
And that's why we need to halve the 
weights. 
So that the intermediate layers can be 
doing geometric averaging of the two 
different models of that layer, from the 
two different restricted Boltzmann 
machines in the original stack. 

