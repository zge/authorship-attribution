In this video, I'm going to describe a 
technique called semantic hashing that 
provides an extremely efficient way of 
finding documents similar to a query 
document. 
The idea is to convert the document into 
a memory address. 
And in that memory to organize things so 
that if you go to a particular address 
and look at the nearby addresses, you'll 
find documents that are very similar. 
This is much like a supermarket where if 
you go to a location where a particular 
product is stored and look around, you'll 
find similar products. 
People have known for a long time that if 
you could get binary descriptors of 
images, you'd have a very good way of 
retrieving images quickly. 
Some binary descriptors are easy to get. 
For example, is it an indoor scene or an 
outdoor scene? 
Is to color image or black and white 
image? 
But it's much harder to get a list of 
say, 30 binary descriptors which are more 
or less orthogonal to one another, which 
is what we really need. 
This is a problem that machine learning 
can help us with. 
We're going to start by looking at the 
equivalent problem for documents, but 
then we're going to apply it to images. 
So consider, instead of getting real 
valued codes for documents, 
getting binary codes, from the word 
cancer documents. 
We do this by training a deep 
auto-encoder that has a logistic units in 
it's code layer. 
That by itself is not sufficient because 
the logistic units will be used in their 
middle ranges where they have real values 
in order to convey as much information as 
possible about the 2,000 word counts. 
To prevent that, we add noise to the 
inputs to the code units during the fine 
tuning stage. So, we first train it as a 
stack of restricted Boltzmann machines. 
We can unroll these Boltzmann machines by 
using the transposes of the white 
matrices for the decoder, and then we 
fine tune it with back propagation. 
And as we're doing that, we add 
additional Gaussian noise to the inputs 
to the code units. 
In order to be resistant to that noise, 
the code units need to be either firmly 
on or firmly off. 
And so the noise will encourage the 
learning to avoid the middle region of 
the logistic where it conveys a lot of 
information, but it's very sensitive to 
noise in its inputs. 
At test time, we simply threshold the 
logistic units in the middle layer to get 
binary values. 
So, if we can train an auto-encoder like 
this, we will be able to convert the 
counts for a bag of words into a small 
number of binary values. 
In other words, we'll have learned a set 
of binary features that are good for 
reconstructing the bag of words. 
Later on, Alex Krizhevsky discovered that 
we don't actually have to add Gaussian 
noise to the inputs to the 30 code units. 
Instead, we can just make them stochastic 
binary units. 
So, during the forward pass, we 
stochastically pick a binary value using 
the output of the logistic. 
And then, during the backward pass, we 
pretend that we've transmitted the real 
value probability from the logistic, 
and that gives us a smooth gradient for 
back propagation. 
Once we've got these short binary codes, 
we could of course do a sequential search 
where for each known document, we store a 
code. 
And then when a query document arrives, 
we first extract its code, if it's not 
one of our known documents, and then we 
compare the code with the codes of all 
the stored documents. 
The comparisons can be very fast, because 
they can use special bit operations on a 
typical CPU which can compare many bits 
in parallel. 
But we have to go through a very long 
list of documents, possibly billions. 
There's a much faster thing we can do, 
there's a much faster thing we can do. 
We can treat the code as if it was a 
memory address. 
So, the idea is that we take a document, 
and we use our deep auto-encoder as a 
hash function that converts a document 
into a 30 bit address Now, we have a 
memory with 30 bit addresses. 
And in that memory, each address will 
have a pointer back to the documents that 
have that address. 
If several documents have the same 
address, we can make a little list there. 
Now, if the auto-ncoder is successful in 
making similar documents have similar 
addresses, we have a very fast way of 
finding similar documents. 
We simply take the query document, you go 
to the address in memory that corresponds 
to its binary code, and then you look at 
nearby addresses. 
In other words, you start flipping bits 
in that address to access nearby 
addresses. 
And you could imagine a little humming 
ball of nearby addresses that differ by 
just a few bits. 
What we expect to find at those nearby 
addresses is semantically similar 
documents. 
So, we've completely avoided searching a 
big list. 
We simply compute a memory address, flip 
a few bits, and look up the similar 
documents. 
It's extremely efficient especially if we 
have a very large database of say, a 
billion documents. 
We've completely avoided the serial 
search through a billion items. 
I sometimes call this supermarket search 
because it's like what you would do in a 
supermarket. 
Suppose you went to an unfamiliar 
supermarket and you wanted to find 
anchovies. 
You might ask the teller at the 
supermarket, where do you keep the cans 
of tuna fish? 
You'd then go to that address in the 
supermarket and you'd look around. 
Hopefully, near there is things like cans 
of salmon and maybe cans of anchovies. 
Of course, if you're unlucky, the 
anchovies might have been stored in a 
completely different place, 
next to the pizza toppings. 
And that's the downside of this kind of 
search. 
Known as supermarket, it's essentially a 
2-D surface. 
So, it's really a 1-D string of shells, 
which have height and that gives you 2-D, 
and so you only have two dimensions in 
which to locate things. And that's not 
sufficient to put all the things you'd 
like to be near one another, near one 
another. 
You'd like, for example, to have the 
vegetarian version of things nearby, or 
the Kosher version of things nearby, or 
the slightly out of date version of 
things nearby. And in 2-D you can't do 
all that. 
But what we have here is a 30 dimensional 
supermarket and that's a hugely more 
complex space where it's very easy to 
have things near an item for many 
different reasons because of similarity 
along many different dimensions. 
Here's another view of what we're doing 
in semantic haching. 
Most of the first retrieval methods work 
by intersecting stored lists that are 
associated with cues extracted from the 
query. 
So, Google, for example, will have a list 
of all the documents that contain some 
particular rare word. 
And when you use that rare word in your 
query, they will immediately have access 
to that list. 
They then have to intersect that list 
with other lists in order to find a 
document that satisfies all the terms in 
your query. 
Now, computers actually have special 
hardware that can intersect 32 very long 
lists in a single machine instruction. 
The hardware is called the memory bus. 
So, each bit in a 32-bit binary address 
specifies a list of half the addresses in 
memory. 
For example, if the bit is on and it's 
the first bit in the address, it 
specifies the top half of memory. 
If the bit is off, it specifies the 
bottom half of memory. 
What the memory bus is doing is 
intersecting 32 lists to find the one 
location that satisfies all 32 values in 
the binary code. 
So, we can think of semantic hashing as a 
way of using machine learning to map the 
retrial problem onto the type of list 
intersection computer's good at. 
As long as our 32-bits correspond to 
meaningful properties of documents or 
images, 
then we can find similar ones very fast 
with no search at all. 

