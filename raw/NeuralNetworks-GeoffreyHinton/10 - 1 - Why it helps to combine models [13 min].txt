In this video, I'm going to talk about the
reason why we want to combine many models
when we're making predictions.
If we have a single model, we have to
choose some capacity for it.
If we choose too little capacity, it would
be able to fit the regularities in the
training data.
And if we choose too much capacity, it
won't be able to fit the sampling error in
the particular training set we have.
By using many models, we can actually get
a better tradeoff between fitting the true
regularities, and overfitting the sampling
error in the data.
At the start of the video,
I'll show you that when you average models
together, you can expect to do better than
any single model.
This effect is largest when the models
make very different predictions from each
other.
And at the end of this video, I'll discuss
various ways in which we can encourage the
different models to make very different
predictions.
As we've seen before, when we have a
limited amount of training data, we tend
to get overfitting.
If we average the predictions of many
different models we can typically reduce
that overfitting.
This helps most when the models make very
different predictions from one another.
For regression, the squared arrow can be
decomposed into a bias term and a variance
term.
And that allows us to analyze what's going
on.
The bias term is big if the model has too
little capacity to fit the data.
It measures how poorly the model
approximates the true function.
The variance term is big if the model has
so much capacity that it's good at
modeling the sampling error in our
particular training set.
So, it's called variance, because if we go
and get another training set of the same
size from the same distribution, our model
will fit differently to that training set,
because it has different sampling error.
And so we'll get variance in the way the
models fit to different training sets.
If we average models together, what we're
doing is we're averaging away the
variance,
And that allows us to use individual
models that have high capacity and
therefore high variance.
These high capacity model typically have
low bias.
So we can get the low bias without
incurring the high variance by using
averaging to get rid of the variance.
So now let's try and analyze how an
individual model compares with an average
of models.
On any one test case some individual
predictors may be better than the combined
predictor.
The different individual predictors will
be better on different cases.
And if the individual predictors disagree
a lot, the combined predictor is typically
better than all of the individual
predictors when we average over test
cases.
So we should aim to make the individual
predictors disagree, without making them
be poor predictors.
The art is to have individual predictors
that make very different errors from one
another, but are each fairly accurate.
So, now let's look at the math and what
happens when we combine networks.
We're going to compare two expected
squared errors.
The first expected squared error is the
one we get if we pick one of the
predictors at random and use that for
making our predictions.
And then what we do is we average overall
predictors, the error we'd expect to get
if we followed that policy.
So Y bar is the average of what all the
predictors say, and YI is what an
individual predictor says.
So Y bar is just the expectation over all
the individual predictors I of YI and I'm
using those angle brackets to represent an
expectation, where the thing that comes
after the angle bracket tells you what
it's an expectation over.
We can write the same thing as one over n
times the sum overall of the n of the yi.
Now, if we look at the expected squared
error we'd get if we chose a predictor at
random,
What we'd have to do is compare that
predictor with the target, take the
squared difference.
And then average that over all predictors.
That's also on the left hand side there.
If I simply add a Y bar and subtract a Y
bar, I don't change the value.
And now it's going to be easier to do some
manipulations.
I can now multiply it that squared and
inside this expectation bracket I have t
minus y bar squared, y I minus y bar
square, and t minus y bar into y I minus y
bar, which has the c will disappear.
So the first term, T minus Y bar squared,
doesn't have an I in it anymore, and so we
can forget about the expectation brackets
for that.
That really is T minus Y bar squared.
And that's the squared arrow you'd get if
you compared the average of the models
with the target.
And our aim is to show the thing on the
left hand side is bigger than that, i.e.,
by using that average, we've reduced the
expected squared error.
So the extra term we have on the right
hand side, is the expectation of y i minus
y bar squared.
And that's just the variance of the y i.
It's the expected squared difference
between y I and y bar.
And then the last tone disappears, it
disappears because the difference of Y I
from Y bar we expect to be uncorrelated
with the difference between the arrow that
the average of the networks makes on the
target.
And so we're multiplying together two
things that are zero mean and uncorrelated
and we expect to get zero on average.
So the result is that the expected squared
error we get by picking a model at random
is greater than the squared error we get
by averaging the models by the variance of
the outputs of the models.
That's how much we win by when we take an
average.
So, I want to show you that in a picture.
So, along the horizontal line, we have the
possible values of the output, and in this
case, all of the different models predict
a value that is too high.
The predictors that are further than
average from T make bigger than average
squared errors, like that bad guy in red,
and the predictors that are less than the
average distance from T make smaller than
average squared arrows.
And the first effect dominates, because
we're using squared error.
So if you look at the math, let's suppose
that the good guy and the bad guy were
equally far from the mean.
So the average squared error they make is
Y bar minus epsilon squared plus Y bar
plus epsilon squared.
And when we work that out, we get the
squared error that the mean of the
predictors makes, plus an epsilon squared.
So we win by averaging predictors before
we compare them with the target.
That's not always true.
It depends very much on using a squared
error.
If, for example, you have a whole bunch of
clocks.
And you try and make them more accurate by
averaging them all,
That'll be a disaster.
And it'll be a disaster because the noise
you expect in clocks isn't Gaussian noise.
What you expect is that, many of them will
be very slightly wrong and a few of them
will have stopped or will be wildly wrong.
And if you average, you make sure they are
all significantly wrong, which is not what
you want.
The same thing applies to the discrete
distribution as we have our class labeled
probabilities.
So suppose that we have two models, and
one gives the correct label of probability
of Pi, and the other gives the correct
label of probability of Pj.
Is it better to pick one model at random,
or it is it better to average those two
probabilities, and predict the average of
Pi and Pj.
What if I had a measure is the log
probability of getting the right answer?
Then, the log of the average of Pi and Pj
is going to be a better bet than the log
of Pi plus the log of Pj averaged.
That's most easily seen in a diagram
because of the shape of the log function.
So that black curve is the log.
On the horizontal access I've drawn Pi and
Pj,
And the gold colored line, joins log Pi to
log Pj.
You can see that if we first start with Pi
and Pj together, to get that average value
at the blue arrow is, and then we compute
the log, we get that blue dot.
Whereas if we first take the log of pi,
and separately take the log of pj, and
then we average those two logs, we get the
mid-point of that gold line,
Which is below the blue dot.
So to make this averaging be a big win, we
want our predictors to differ by a lot.
And there's many different ways to make
them differ.
You could just rely on a learning
algorithm that doesn't work too well, and
get stuck in different local optima each
time.
It's not a very intelligent thing to do,
but it's worth a try.
You could use lots of different kinds of
models, including ones that are not neural
networks.
So, it makes sense to try decision trees,
Gaussian process models, support vector
machines.
I'm not explaining any of those in this
course.
In Andrew Ng's machine on Coursera, you
can learn about all those things.
Well you could try many other different
kinds of model.
If you really want to use a bunch of
different neural-network models, you can
make them different by using a different
number of hidden layers or a different
number of units per layer or different
types of unit.
Like in some nets you could use
rectified-linear units,
And in other nets you could use logistic
units.
You could use different types or strengths
of weight penalty.
So you might use early stopping for some
nets, and an L2 weight penalty for others,
and an L1 weight penalty for others.
You could use different learning
algorithms.
So for example you could use full batch
for some, and mini batch for others, if
your data set is small enough to allow
that.
You can also make the models differ by
training the models on different training
data.
So, there's a method introduced by Leo
Breiman called bagging, where you train
different models on different subsets of
the data.
And you get these subsets by sampling the
training set with replacement.
So we sampled a training set that had
examples A, B, C, D, and E.
And we got five examples, but we'll have
some missing and some duplicated.
And we train one of our models on that
particular training set.
This is done in a method called random
forest that uses bagging with decision
trees, which Leo Breiman was also involved
in inventing.
When you train decision trees with bagging
and then average them together, they work
much better than single decision tree bys
themselves.
In fact, the connect box uses random
forests to convert information about depth
into information about where your body
parts are.
We could use bagging with neural nets, but
it's very expensive.
If you wanted to train say, twenty
different neural nets this way, you'd have
to get your twenty different training
sets.
And then it would take twenty times as
long as training one net.
That doesn't matter with decision tress
cuz they're so fast to train.
Also, at test time, you'd have to run
these twenty different nets.
Again, with decision trees, that doesn't
matter, cuz they're so fast to use at test
time.
Another method for making the training
data different is to train each model on
the whole training set,
But to weight the cases differently So, in
boosting, we typically we use a sequence
of fairly low capacity models.
And we weight the training cases for each
model differently.
What we do is we up weight the cases the
previous model got wrong and we down
weight the case of previous model got
right.
So the next model in the sequence doesn't
waste its time trying to model cases that
are already correct.
It uses its resources to try to deal with
cases the other models are getting wrong.
An early use of boosting, was with neural
nets for MNIST,
And there when computer's are actually
slower.
One of the big advantage is was that it
focused to competitional resources on
modelling the tricky cases,
And didn't waste a lot of time, going over
easy cases again and again.
