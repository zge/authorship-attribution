In this video, I'm going to talk about 
applying deep autoencoders to document 
retrieval. 
There was a method developed some time 
ago called latent semantic analysis, that 
amounts to applying principal components 
analysis to vectors of word counts 
extracted from documents. 
The codes produced by latent semantic 
analysis can then be used for judging 
similarity between documents so they can 
be used for document retrieval. 
Obviously, if deep autoencoders work much 
better than PCA, we would expect to be 
able to extract much better codes using a 
deep autoencoder than using latent 
semantic analysis. 
And Rus-lan Salakhutdinov and I showed 
that, that was indeed the case. 
Using a big database of documents, we 
showed that ten components extracted with 
a deep autoencoder are actually worth 
more than 50 components extracted with a 
linear method, like latent semantic 
analysis. 
We also showed that if you make the code 
very small, having just two components, 
you can use those two components for 
visualizing documents as a point in a 
two-dimensional map. 
And this, again, works much better than 
just extracting the first two principal 
components. 
To find documents that are similar to a 
query document, 
the first thing we do is convert each 
document into a big bag of words. 
In other words, we have a vector of word 
counts that ignores the order of the 
words. 
This clearly throws away quite a lot of 
information. 
But it also retains a lot of information 
about the topic of the document. 
We ignore words like the or over" which 
are called stop words," because they 
don't have much information about the 
topic. 
So if you look on the right, I've done 
the counts for various words, and they're 
actually the counts for the document on 
the left. 
So, if you look at what words we have 
nonzero counts for, 
they are vector, and count, and query, 
and reduce, and bag, and a word, 
that tells you quite a lot about what the 
document is about. 
Now, we could compare the word counts of 
the query document with the word counts 
of millions of other documents. 
But that would involve comparing quite 
big vectors. 
In fact, we use vectors of size 2000. 
So, that would be slow. 
Alternatively, we could use each query 
vector to a much smaller vector that 
still contained most of the information 
about the content. 
So, here's how we do the reduction. 
We take the deep autoencoder and we 
compress the 2,000-word counts down to 
ten real numbers, from which we can 
reconstruct the 2,000-word counts, 
although we can't reconstruct them very 
well. 
We train the neural network to reproduce 
its input vector as its output vector as 
well as possible. 
And that forces it to put as much 
information about the input into those 
ten numbers as possible. 
We can then compare documents using just 
ten numbers. 
That's going to be much faster. 
So, there's one problem with this, which 
is word counts aren't quite the same as 
pixels or real values. 
What we do is we divide the counts in a 
bag of words by the total number of 
non-stop words and that converts the 
vector of counts into a probability 
vector where the numbers add up to one. 
You can think of it as the probability of 
getting a particular word if you picked a 
word at random in the document, as long 
as that is not a stop word. 
So, the output of the autoencoder, we're 
using a great, big 2,000-way softmax. 
And our target values are the 
probabilities of words when we reduce the 
count vector to a probability factor. 
There's one further trick we have to do. 
We treat the word counts as probabilities 
when we're reconstructing them. 
But when we're using them to activate the 
first hidden layer, we multiply all the 
weights by N. 
And that's because we have N different 
observations from that probability 
distribution. 
If we left them as probabilities, the 
input units would have very small 
activities and wouldn't provide much 
input to the first hidden layer. 
So, we have this funny property that for 
the first restricted Boltzmann machine, 
the bottom up weights, are N times bigger 
than the top down weights. 
So, how well does this work? 
We trained using bags of 2,000 words on 
4,000 business documents from the Reuters 
data set. 
And these documents had been hand 
labelled with about a 100 different 
categories. 
We first trained a stack of restricted 
Boltzmann machines, and then, we fine 
tuned with back propagation using a 
2,000-way softmax as the output. 
And then, we test it on a different set 
of 4,000 documents. 
And to test, you pick one document to be 
the query, one of the test documents, 
and then you rank order all the other 
test documents by using the cosine of the 
angles between the ten-dimensional 
vectors that the ordering code gives you. 
You repeat this for each of the 4,000 
possible test documents and then you plot 
the number of documents you're going to 
retrieve, that is how far down that rank 
list you're going to go, against the 
proportion that are in the same hand 
labelled class as the query document. 
This is not a very good measure of the 
quality of the retrieval. 
But we're going to use the same measure 
for comparing the LSA. 
And so, at least, it's a fair comparison. 
So, here's the accuracy of the retrieval 
as a function of the number of retrieved 
documents. 
When you see that an autoencoder was just 
using a code with ten real numbers is 
doing better than latent emantic 
analysis, using 50 real numbers. 
And, of course, it's five times less work 
per document after you've got the code. 
Latent semantic analysis with ten real 
numbers is much worse. 
We can also do the same thing where we 
reduce to two real numbers, and then, 
instead of doing retrieval, we're just 
going to plot all the documents in a map 
but we're going to color that 
two-dimensional point that corresponds to 
the two numbers produced by PCA by the 
class of the document. 
So, we took the major classes of the 
documents. 
We gave those major classes different 
colors. 
And then, we used PCA on log of one plus 
the count. 
The point of doing that is that it 
suppresses counts with very big numbers 
which tends to make PCA work better. 
This is the distribution you get. 
As you can see, there is some separation 
of the classes. 
The green class is in one place. 
The red class is in a slightly different 
place. 
But the classes are very mixed up. 
Then, we did the same thing by using a 
deep autoencoder to reduce the documents 
to two numbers, and, again, plotting the 
documents in a two-dimensional space 
using those two numbers as the 
coordinates. 
And here's what we got. 
It's a much better layout. 
It tells you much more about the 
structure of the data set. 
You can see the different classes, and 
you can see that they're quite 
well-separated. 
We assume that the documents in the 
middle are ones which didn't have many 
words in them, and therefore, it was hard 
to distinguish between the classes. 
A visual display like this could be very 
useful. If, for example, you saw one of 
green dots was the accounts and earning 
reports from Enron, you probably wouldn't 
want to buy shares in a company that has 
a green dot nearby. 

