In this video, I'm going to describe a new
way of combining a very large number of
neural network models without having to
separately train a very large number of
models.
This is a method called dropout that's
recently been very successful in winning
competitions.
For each training case, we randomly omit
some of the hidden units.
So, we end up with a different
architecture for each training case.
We can think of this as having a different
model for every training case.
And then, the question is, how could we
possibly train a model on only one
training case and how could we average all
these models together efficiently at test
time?
The answer is that we use a great deal of
weight sharing.
I want to start by describing two
different ways of combining the outputs of
multiple models.
In a mixture, we combine models by
averaging their output probabilities.
So, if model A assigns probabilities of
0.3, 0.2 and 0.5, to three different
answers, model B assigns probabilities of
0.1, 0.8 and 0.1, the combined model
simply assigns the averages of those
probabilities.
A different way of combining models is to
use a product of the probabilities. Here,
we take a geometric mean of the same
probabilities.
So, model A and model B again assign the
same probabilities as they did before.
But now, what we do is we multiply each
pair of probabilities together and then
take the square root.
That's the geometric mean and the
geometric means will generally add up to
less than one.
So, we have to divide by the sum of the
geometric means to normalize the
distribution so that it adds up to one
again.
You'll notice that in a product, a small
probability output by one model, has veto
power over the other models.
Now I want to describe an efficient way to
average a large number of neural nets that
gives us an alternative to doing the
correct Bayesian thing.
The alternative probably doesn't work
quite as well as doing the correct
Bayesian thing, but it's much more
practical.
So, consider the neural net with one
hidden layer, shown on the right.
Each time we present a training example to
it,
What we're going to do is randomly emit
each hidden unit with a probability of
0.5.
So, we crossed out three of the hidden
units here.
And we run the example through the net
with those hidden units absent.
What this means is that we're randomly
sampling from two to the h architectures,
where h is the number of hidden units,
It's a huge number of architectures.
Of course, all of these architectures show
weights.
That ism whenever we use a hidden unit,
it's got the same weight as it's got in
other architectures.
So, we can think of dropout as a form of
model averaging.
We sample from these two to the h models.
Most of the models, in fact, will never be
sampled.
And a model of this sampled only gets one
training example.
That's a very extreme form of bagging.
The training sets are very different for
the different models, but they're also
very small.
The sharing of the weights between all the
models means that each model is very
strongly regularized by the others.
And this is a much better regularizer than
things like L2 or L1 penalties.
Those penalties pull the weights toward
zero.
By sharing weights with other models, a
models gets regularized by something
that's going to tend to pull the weight
towards the correct value.
The question still remains what we do with
test time.
So, we could sample many of the
architectures, maybe a hundred, and take
the geometric mean of the output
distributions.
But that would be a lot of work.
There's something much simpler we can do.
We use all of the hidden units, but we
halve their outgoing weights.
So, they have the same expected effect as
they did when we were sampling.
It turns out that using all of the hidden
units with half their outgoing weights,
exactly computes the geometric mean that
the predictions that all two to the h
models would have used, provided we're
using a softmax output group.
If we have more than one hidden layer, we
can simply use drop out at 0.5 in every
layer.
At test time, we halve all the outgoing
weights of hidden units,
And that gives us what I call the mean
net.
So, we use a net that has all of the units
but the weights are halved.
When we have multiple hidden layers, this
is not exactly the same as averaging lots
of set per dropout model, but it's a good
approximation and it's fast.
We could run lots of stochastic models
with dropout, and then average across
those stochastic models.
And that would have one advantage over the
mean net.
It would give us an idea of the
uncertainty in the answer.
What about the input layer?
Well, we can use the same trick there,
too.
We use dropout on the inputs, but we use a
higher probability of keeping an input.
This trick's already in use in a system
called denoising autoencoders, developed
by Pascal Vincent, Hugo Laracholle and
Yoshua Bengio at the University of
Montreal, and it works very well.
So, how well does dropout work?
Well, the record breaking object
recognition net developed by Alex
Krizhevsky would have broken the record
even without dropout.
But it broke a lot more by using dropout.
In general, if you have a deep neural net
and it's overfitting dropout, it will
typically reduce the number errors by
quite a lot.
I think any net that requires early
stopping in order to prevent it
overfitting would do better by using
dropout. It would, of course, take longer
to train and it might mean more hidden
units.
If you got a deep neural net and it's not
overfitting, you should probably be using
a bigger one and using dropout, that's
assuming you have enough computational
power.
There's another way to think about
dropout, which is how I originally arrived
at the idea.
And you'll see it's a bit related to
mixtures of experts, and what's going
wrong when all the experts cooperate,
What's preventing specialization?
So, if a hidden unit knows which other
hidden units are present, it can co-adapt
to the other hidden units on the training
data.
What that means is, the real signal that's
training a hidden unit is, try to fix up
the error that's leftover when all the
other hidden units have had their say.
That's what's being back propagated to
train the weights of each hidden unit.
Now, that's going to cause complex
co-adaptations between the hidden units.
And these are likely to go wrong when
there's a change in the data.
So, a new test data,
If you rely on a complex co-adaptation to
get things right on the training data,
it's quite likely to not work nearly so
well on new test data.
It's like the idea that a big, complex
conspiracy involving lots of people is
almost certain to go wrong because there's
always things you didn't think of.
And if there's a large number of people
involved, one of them will behave in an
unexpected way.
And then, the others will be doing the
wrong thing.
It's much better if you want conspiracies,
to have lots of little conspiracies.
Then, when unexpected things happen, many
of the little conspiracies will fail, but
some of them will still succeed.
So, by using dropout,
We force a hidden unit to work with
combinatorially many other sets of hidden
units.
And that makes it much more likely to do
something that's individually useful
rather than only useful because of the way
particular other hidden units are
collaborating with it.
But it is also going to tend to do
something that's individually useful and
is different from what other hidden units
do.
It needs to be something that's marginally
useful, given what its co-workers tend to
achieve.
And I think this is what's giving nets
with dropout, their very good performance.
