In this video, I'll talk about a 
different way of learning sigmoid belief 
notes. 
This different method arrived in an 
unexpected way. 
I stopped working on sigmoid belief nets 
and went back to Boltzmann machines. 
And discovered that restricted Boltz 
machines could actually be learned fairly 
efficiently. 
Given that a restricted Boltzmann machine 
could efficiently learn a layer of 
nonlinear features. It was tempting to 
take those features, treat them as data, 
and apply another restricted Boltzmann 
machine to model the correlations between 
those features. 
And one can continue like this, stacking 
one Boltzmann machine on top of the next 
one to learn lots of layers of nonlinear 
features. 
This eventually led to a big resurgence 
of interest in deep neural nets. 
The issue then arose. Once you stacked up 
lots of restricted Boltzmann machines, 
each which is learned by modeling the 
patterns of future activities produced by 
the previous Boltzmann machines. 
Do you just have a set of separate 
restricted Boltzmann machines or can they 
all be combined together into one model? 
Now, anybody sensible would expect that 
if you combined a set of restricted 
Boltzmann machines together to make one 
model, what you'd get would be a 
multilayer Boltzmann machine. 
However, a brilliant graduate student of 
mine called G.Y. 
Tay, figured out that that's not what you 
get. 
You actually get something that looks 
much more like a sigmoid belief net. 
This was a big surprise. 
It was very surprising to me that we'd 
actually solved the problem of how to 
learn deep sigmoid belief nets by giving 
up on it and focusing on learning 
undirected models like Boltzmann 
machines. 
Using the efficient learning algorithm 
for restricted Boltzmann machines. 
It's easy to train a layer of features 
that receive input directly from the 
pixels. 
We can treat the patterns of activation 
of those feature detectors as if they 
were pixels, 
and learn another layer of features in a 
second hidden layer. 
We can repeat this as many times as we 
like with each new layer of features 
modelling the correlated activity in the 
features in the layer below. 
It can be proved that each time we add 
another layer of features, we improve a 
variational lower bound on the log 
probability that some combined model 
would generate the data. 
The proof is actually complicated, and it 
only applies if you do everything just 
right, 
which you don't do in practice. 
But, the proof is very reassuring, 
because it suggests that something 
sensible is going on when you stack up 
restricted Boltzmann machines like this. 
The proof is based on a neat equivalence 
between a restricted bolson machine and 
an infinitely deep belief net. 
So here's a picture of what happens when 
you learn two restricted Boltzmann 
machines, one on top of the other, 
and then you combine them to make one 
overall model, which I call a deep belief 
net. 
So first we learn one Boltzmann machine 
with its own weights. 
Once that's been trained, we take the 
hidden activity patterns of that 
Boltzmann machine when it's looking at 
data and we treat each hidden activity 
pattern as data for training a second 
Boltzmann machine. 
So we just copy the binary states to the 
second Boltzmann machine, and then we 
learn another Boltzmann machine. 
Now one interesting thing about this, is 
that if we start the second Boltzmann 
machine off with W2 being the transpose 
of W1, and with as many hidden units in 
h2 as there are in v, then the second 
Boltzmann machine will already be a 
pretty good model of h1, 
because it's just the first model upside 
down. 
And for a restricted Boltzmann machine, 
it doesn't really care which you call 
visible and which you call hidden. 
It's just a bipartite graph that's 
learned to model. 
After we've learned those two Boltzmann 
machines, we're going to compose them 
together to form a single model and the 
single model looks like this. 
Its top two layers adjust the same as the 
top restricted Boltzmann machine. 
So that's an undirected model with 
symmetric connections, but its bottom two 
layers are a directed model like a 
sigmoid belief net. 
So what we've done is we've taken the 
symmetric connections between v and h1 
and we've thrown away the upgoing part of 
those and just kept the dangering part. 
To understand why we've done that is 
quite complicated and that will be 
explained in video 13F. 
The resulting combined model is clearly 
not a Boltzmann machine, because its 
bottom layer of connections are not 
symmetric. 
It's a graphical model that we call a 
deep belief net, where the lower layers 
are just like sigmoid belief nets and the 
top two layers form a restricted 
Boltzmann machine. 
So it's a kind of hybrid model. 
If we do it with three Boltzmann machines 
stacked up, we'll get a hybrid model that 
looks like this. 
The top two layers again are a restricted 
Boltzmann machine and the layers below 
are directed layers like in a sigmoid 
belief net. 
To generate data from this model the 
correct procedure is, 
first of all, you go backwards and 
forwards between h2 and h3 to reach 
equilibrium in that top level restricted 
Boltamann machine. 
This involves alternating Gibbs sampling, 
where you update all of the units in h3 
in parallel, and update all of the units 
in h2 in parallel, 
then go back and update all of the units 
in h3 in parallel. And you go backwards 
and forwards like that for a long time 
until you've got an equilibrium sample 
from the top-level restricted Boltamann 
machine. 
So the top-level restricted Bolson 
machine is defining the prior 
distribution of h2. 
Once you've done that, you simply go once 
from h2 to h1 using the generative 
connections w2. 
And then, whatever binary patent you get 
in h1, you go once more to get generated 
data, using the weights w1. 
So we're performing a top-down pass from 
h2, to get the states of all the other 
layers, 
just like in a sigmoid belief net. 
The bottom-up connections, shown in red 
at the lower levels, are not part of the 
generative model. 
They're actually going to be the 
transposes of the corresponding weights. 
So they're the transpose of w1 and the 
transpose of w2, 
and they're going to be used for 
influence, but they're not part of the 
model. 
Now, before I explain why stacking up 
Boltzmann machines is a good idea, I need 
to sort out what it means to average two 
factorial distributions. 
And it may surprise you to know that if I 
average two factorial distributions, I do 
not get a factorial distribution. 
What I mean by averaging here is taking a 
mixture of the distributions, so you 
first pick one of the two at random, and 
then you generate from whichever one you 
picked. 
So, you don't get a factorial 
distribution. 
Suppose we have an RBM with 4 hidden 
units and suppose we give it a visible 
vector. 
And given this visible vector, the 
posterior distribution over those 4 
hidden units is factorial. 
And lets suppose the distribution was 
that the first and second units have a 
probability of 0.9 of turning on and the 
last two have a probability of 0.1 of 
turning on. 
What it means for this to be factorial is 
that, for example, the probability that 
the first two units were both be on in a 
sample from this distribution, is exactly 
0.81. 
Now suppose we have a different angle 
vector v2, and the posterior distribution 
over the same 4 hidden units is now 0.1, 
0.1, 0.9, 0.9, which I chose just to make 
the math easy. 
If we average those two distributions, 
the mean probability of each hidden unit 
being on, is indeed, the average of the 
means for each distribution. 
So the means are 0.5, 0.5, 0.5, 0.5, 
but what you get is not a factorial 
distribution defined by those 4 
probabilities. 
To see that, consider the binary vector 
1, 1, 0, 0 over the hidden units. 
In the posterior for v1, 
that has a probability of 0.9^4, because 
it's 0.9 * 0.9 * 1 - 0.1 * 1 - 0.1. 
So that's 0.43. 
In the posterior for v2, this vector is 
extremely unlikely. 
It has a probability of 1 in 10,000. 
If we average those two probabilities for 
that particular vector, we'll get a 
probability of 0.215, 
and that's much bigger than the 
probability assigned to the vector 1, 1, 
0, 0 by factorial distribution with means 
of 0.5. 
That probability will be 0.5^4, which is 
much smaller. 
So, the point of all this, is that when 
you average two factorial posteriors, you 
get a mixture distribution that's not 
factorial. 
Now, let's look at why the greedy 
learning works. 
That is why it's a good idea to learn one 
restricted Boltzmann machine. 
And then learn a second restricted 
Boltzmann machine that models the 
patterns of activity in the hidden units 
of the first one. 
The weights of the bottom level 
restricted Boltzmann machine, actually 
define four different distributions. 
Of course, they define them in a 
consistent way. 
So the first distribution is the 
probability of the visible units given 
the hidden units. 
And the second one is the probability of 
the hidden units given the visible units. 
And those are the two distributions we 
use for running our alternating mark of 
chain that updates the visibles given the 
hiddens and then updates the hiddens 
given the visibles. 
If we run that chain long enough, we'll 
get a sample from the joint distribution 
of v and h. 
And so the weights clearly also define 
the joint distribution. 
They also define the joint distribution 
more directly in terms of E to the minus 
the energy, 
but for nets with a large number of 
units, we can't compute that. 
If you take the joint distribution, 
p(v|h), and you just ignore v, we now a 
distribution for h. 
That's the prior distribution over h, 
defined by this restricted Boltzmann 
machine. 
And similarly, if we ignore h, we have 
the prior distribution over v, defined by 
the restricted Boltzmann machine. 
And now, we're going to pick a rather 
surprising pair of distributions from 
those four distributions. 
We're going to define the probability 
that the restricted Boltzmann machine 
assigns to a visible vector v as the sum 
over all hidden vectors of the 
probability it assigns to h times the 
probability of v given h. 
This seems like a silly thing to do, 
because defining p(h) is just as hard as 
defining p(v). 
And nevertheless, we're going to define 
p(v) that way. 
Now, if we now leave p(v|h) alone, 
but learn a better model of p(h), 
that is, learn some new parameters that 
give us a better model of p(h) and 
substitute that in instead of the old 
model we had of p(h). 
We'll actually improve our model of v. 
And what we mean by a better model of 
p(h) is a prior over h that fits the 
aggregated posterior better. 
The aggregated posterior is the average 
over all vectors in the training set of 
the posterior distribution over h. 
So, what we're going to do, is use our 
first RBM to get this aggregated 
posterior and then use our second RBM to 
build a better model of this aggregated 
posterior than the first RBM has. 
And if we start the second RBM off as the 
first one upside down, it will start with 
the same model of the aggregated 
posterior as the first RBM has. 
And then, if we change the weights we can 
only make things better. 
So, that's an explanation of what's 
happening when we stack up RBMs. 
Once we've learned to stack up Boltzmann 
machines, then combine them together to 
make a deep belief net, 
we can then actually fine-tune the whole 
composite model using a variation of the 
wake-sleep algorithm. 
So we first learn many layers of features 
by stacking up IBMs. 
And then we want to fine-tune both the 
bottom-up recognition weights and the 
top-down generative weights to get a 
better generative model and we can do 
this by using three different learning 
routes. 
First, we do a stochastic bottom-up pass, 
and we adjust the top down generative 
weights of the lower layers to be good at 
reconstructing the feature activities in 
the layer below. 
That's just as in the standard wake-sleep 
algorithm Then, in the top level RBM, we 
go backwards and forwards a few times, 
sampling the hiddens of that RBM, and the 
visibles of that RBM, and the hiddens of 
the RBM, and so on. 
So that's just like the learning 
algorithm for RBMs. 
And having done a few iterations of that, 
we do contrastive divergence learning. 
That is, we update the weights of the RBM 
using the difference between the 
correlations when activity first got to 
that RBM and the correlations after a few 
iterations in that RBM. 
We take that difference and use it to 
update the weights. 
And then, the third stage, we take the 
visible units of that top-level RBM by 
its lower level units. 
And starting there, we do a top-down 
stochastic pass, using the directed lower 
connections, which are just a sigmoid 
belief net. 
Then, having generated some data from 
that sigmoid belief net, we adjust the 
bottom up rates to be good at 
reconstructing the feature activities in 
the layer above. 
So that's just the sleep phase of the 
wake-sleep algorithm. 
The difference from the standard 
wake-sleep algorithm is that that 
top-level RBM acts as a much better prior 
over the top layers, than just a layer of 
units which are assumed to be 
independent, which is what you get with a 
sigmoid belief net. 
Also, rather than generating data by 
sampling from the prior, what we're 
actually doing is looking at a training 
case, going up to the top-level RBM and 
just running a few iterations before we 
generate data. 
So now we're going to look at an example 
where we first learn some RBMs, stacking 
them up, 
and then we do contrastive wake-sleep to 
fine-tune it, 
and then we look to see what it's like. 
Is it a generative model? 
And also if we're recognizing things. 
So first of all, we're going to use 500 
binary hidden units to learn to model all 
10 digit classes in images of 28 by 28 
pixels. 
Once we've learned that RBM, without 
knowing what the labels are, 
so it's unsupervised learning. 
We're going to take the patterns of 
activity in those 500 hidden units that 
they have when they're looking at data. 
We're going to treat those patterns of 
activity as data and we're going to learn 
another RBM that also has 500 units, 
and those two are learned without knowing 
what the labels are. 
Once we've done that we'll actually tell 
it the labels. 
So the first two hidden layers are 
learned without labels, 
and then, we add a big top layer and we 
give it the 10 labels. 
And you can think that we concatenate 
those 10 labels with the 500 units that 
represent features, 
except that the 10 labels are really one 
soft match unit. 
Then we train that top-level RBM to model 
the concatenation of the soft match unit 
for the 10 labels with the 500 feature 
activities that were produced by the two 
layers below. 
Once we've trained the top-level RBM, we 
can then fine-tune the whole system by 
using contrastive wake-sleep. 
And then we'll have a very good 
generative model and that's the model 
that I showed you in the intro video. 
So if you go back, and you find the 
introduction video for this course, 
you'll see what happens when we run that 
model. 
You'll see how good it is at recognition 
and you'll also see that it's very good 
at generation. 
In that introductory video, I promised 
you, you would eventually explain how it 
worked, 
and I think you've now seen enough to 
know what's going on when this model is 
learned. 

