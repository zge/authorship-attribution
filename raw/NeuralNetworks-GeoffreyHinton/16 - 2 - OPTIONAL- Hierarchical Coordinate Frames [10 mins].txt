In this video, I am going to describe how 
we might be able to combine recognition 
of objects by using the relationships 
between their parts and recognition of 
objects using a deep neural network. 
At present in computer vision, there's 
broadly three approaches to recognizing 
objects. 
Either you use a deep convolutional 
neural net and this currently works best. 
Or you use a parts-based approach, which 
I think in the long run is going to work 
best,. Or you use the existing features 
that computer vision people know how to 
extract from images and make histograms 
of them and then use lots of hand 
engineering. 
This is the approach that the 
convolutional neural networks have 
recently beaten. 
The point of this video is to show how we 
might combine a parts-based approach with 
early stages that use convolutional 
neraul networks. 
Even though convolutional neural networks 
have worked very well for recognizing 
objects and images, 
I think there's something missing. 
When we pool the activities of a bunch of 
replicator feature detectors, we lose the 
precise position of the feature detector 
that was most active. 
This means we don't know exactly where 
things are and that's fatal for high 
level parts, such as the nose and the 
mouth. 
In order to recognize whose face it is, 
you need to use the precise spatial 
relationships between high level parts, 
like noses and mouths. 
If you overlap the pools so that each 
feature occurs in several different 
pools, you retain more information about 
its position and that makes things work a 
bit better, but I don't think that's the 
answer. 
A related problem is that convolutional 
neural nets that use translations to 
replicate feature detectors cannot 
extrapolate their understanding of 
geometrical relationships to radically 
new viewpoints like different 
orientations or different scales. 
We could, of course, try replicating 
across orientation and scale, but then, 
we get huge numbers of replicated feature 
detectors. 
Now, people are very good at 
extrapolating. 
After seeing a new shape once, they can 
recognize it from a very different 
viewpoint. 
Currently, the way we deal with that with 
convolutional neural networks is to train 
them on transformed data. 
So this involves giving them huge 
training sets, where we tried to 
transform the data through different 
orientations, and scales, and lightings, 
and all sorts of other things so that the 
network can cope with those variations. 
But that's a very clumsy way of dealing 
with the variations. 
I think a much better way is to use a 
hierarchy of coordinate frames and to use 
a group of neurons to represent the 
conjunction of the shape of a feature and 
it's pose relative to the retina. 
So when these neurons are active, it 
tells you a feature of that kind is 
there, like a nose. 
And the precise activities or relative 
activities of these neurons tell you the 
pose of the nose. 
If you think about representing the pose 
of something, that's really a 
relationship between two coordinate 
frames. 
So it's a relationship between a 
coordinate frame embedded in the thing 
and the coordinate frame of the camera or 
retina. 
So, in order to represent the pose of 
something, we have to embed a coordinate 
frame within it. 
Once we've done this and we have a 
representation of the pose of thoughts of 
objects relative to the reckoner, it's 
easy to use the relationships between 
paths to recognize larger objects. 
So we're going to use the consistency of 
the poses of the parts as a cue for 
recognizing a larger shape. 
If you look at this picture, we have a 
nose and we have a mouth and then the 
right spacial relationship to one 
another. 
One way of thinking about that is that if 
you ask the mouth to predict the pose of 
the whole face, and if you ask the nose 
to predict the pose of the whole face, 
they'll make similar predictions. 
If you look on the right share, we have 
the same nose and the same mouth, but now 
they're in the wrong spatial 
relationship. 
And that means that if they separately 
make predictions about the pose of the 
whole face, those predictions won't agree 
at all. 
So here's two layers in a hierarchy of 
parts, where the larger parts can be 
recognized by consistent predictions from 
smaller parts. 
Let's suppose we're looking for a face. 
And so in the middle here the ellipse 
with Tj in it is a collection of neurons 
that are going to be used for recognizing 
the pose of the face. 
And the Pj next to it is a single 
logistic neuron that's going to be used 
for representing whether or not we think 
there's a face there. 
We have a similar representation in the 
lab below, where we have a representation 
of the pose for a mouth and a 
representation of the pose for a nose, 
and we can recognize the phase by 
noticing that those two representations 
make consistent predictions. 
So we take a vector of activities that 
represents the pose of the mouth. 
We multiply by matrix Tij that represents 
the spatial relationship between a mouth 
and a face, and we get a prediction, Ti * 
Tij for the pose of the face. 
We do the same thing with the nose. 
We take a vector of neural activities 
that represents the pose of the nose, Th. 
We multiply it by the relationship 
between the nose and the face and we get 
another prediction for the pose of the 
face. 
If those two predictions agree, there's a 
face there, because the nose and the 
mouth are in the right spacial 
relationship and that's very unlikely 
without there being a face there. 
What we're doing here is inverse computer 
graphics. 
In computer graphics, if you knew the 
pose of the face, you could now compute 
by using the inverse of Tij, the pose of 
the mouth and similarly for the nose. 
So in computer graphics you're going from 
poses of larger things to poses of their 
parts. 
In computer vision, you need to go from 
the poses of the parts to the poses of 
larger things and you need to check 
consistency when you do that. 
Now if we can get a neural net to 
represent these pose vectors, as vectors 
of neural activity, then we get a very 
nice property. 
Spatial relationships can then be 
modelled as linear operations. 
That makes it very easy to learn 
hierarchy visual entities, 
and it also makes it very easy to 
generalize across viewpoints. 
So, what's going to happen when we make 
small changes in viewpoint is the pose 
vectors, those vectors of neural 
activities are all going to change. 
What's going to be invariant is the 
weights. 
It was the weights that represented the 
relationship between the part from the 
hole, like Tij on the previous slide, and 
those don't depend on viewpoint. 
So we want to get the invariant 
properties of a shape into the weights. 
And we want to have the pose vectors in 
the activities, 
because when we change viewpoint, all 
those pose vectors are going to change. 
So, rather than trying to get neural 
activities that are invariant to 
viewpoint, which is what the pooling in 
the convolutional net is trying to do. 
We're going to aim to get neural 
activities that are equivariant of 
viewpoint. 
As the pose of the object varies the 
activities of the neurons vary. 
That means the percept of an object, not 
its label but what it looks like, is 
going to change as the viewpoint changes. 
I'm going to finish by giving you some 
evidence that our visual systems really 
do impose coordinate frames in order to 
represent shapes. 
This was pointed out a long time ago by a 
great psychologist called Irvin Rock. 
So if you look at this shape and I tell 
you it's a country, most people don't 
know which country it is. 
They look at it, and they think it looks 
a bit like Australia, but it's so sort, 
sort of mirror image of Australia, 
but it's not really a familiar country at 
all. 
If you tell them where the top is, that 
it's that way up, they immediately 
recognize that's it's Africa. 
If they knew what coordinate frame to 
impose on it, it immediately becomes a 
familiar shape. 
Similarly, if I give you a shape like 
this one, you can perceive it as a square 
or you can perceive it as a diamond. 
Those are two completely different 
percepts. 
What you know about the shape is totally 
different depending on which way you 
perceive it. 
So for example, if you perceive it as a 
tilted square, you're acutely sensitive 
to whether the angles are right angles. 
If you perceive it as an upright diamond, 
you're not sensitive to that at all. 
The angles could be 5 degrees off and you 
wouldn't notice it, but you are sensitive 
to something else. 
If you perceive it as an upright diamond, 
you're acutely sensitive to whether the 
corner on the left and the corner of the 
right are at the same height. 
And you'll probably notice now that, in 
this figure, they're very slightly 
different heights. 
These kind of demonstrations are 
evidence, that in order to represent 
shapes we impose coordinate frames on 
them. 
Because when you're looking at that 
square or diamond, it's the same thing 
you're looking at, but the percept's 
totally different depending on what 
coordinate frame you impose. 

