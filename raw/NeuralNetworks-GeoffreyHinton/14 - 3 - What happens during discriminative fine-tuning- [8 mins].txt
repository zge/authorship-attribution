In this video we'll look in more detail 
at what happens when a neural network is 
discriminatorily fine tuned after it's 
first been pre-trained as a stack of 
Boltzmann machines. 
What we'll see is that the weights in the 
lower layers hardly change at all. 
But that nevertheless these tiny changes 
make a big difference in the 
classification performance of the neural 
net, because they put the decision 
boundaries in the right places. 
We also see that the effect of 
pre-training is to make deeper networks 
more effective than shallower ones. 
Without pre-training it's often the other 
way around. 
Finally, I give a fairly general argument 
about why it makes sense to start by 
doing generative training. 
And only after this is well under way to 
consider discriminative training. 
So now we're going to look at some work 
done in Yoshua Bengio's lab, examining 
what happens during fine tuning after a 
net's been generatively pre-trained. 
If you look on the left, there's the 
receptive fields in the first hidden 
layer of feature detectors, after the 
generative pre-training but before the 
fine tuning. 
Then on the right, there's the same 
receptive fields after the fine-tuning 
then you'll see almost nothing has 
changed. 
Nevertheless, the changes helped with 
discrimination. 
Here's an example of how pre-training 
reduces the test errors for networks with 
one hidden left. 
The task was discriminating between 
digits in a very large set of distorted 
digits. 
And you can see that after the back 
propagation fine tuning, the networks 
with pre-training almost always did 
better than the networks without 
pre-training. 
The effect gets even bigger, if you use 
deeper networks. 
So here you can see that there's 
basically no overlap between the two 
distributions. 
And the deep networks with pre-training 
have got better than the shallow 
networks, and the deep networks without 
pre-training have got worse than the 
shallow networks. 
This is showing you the classification 
error and the variation classification 
error as you change the number of layers 
when you're not doing pre-training. 
And you can see that two layers appears 
to be best. 
And by the time you've got four layers, 
you're doing considerably worse. 
By contrast, if you use pre-training, 
four layers is better than two layers. 
There's much less variation and DR is 
lower. 
This is a visualization made with Teeson 
of what happens to the weights during 
training for both pretrained and 
non-pretrained networks and they are all 
plotted in the same space but you can see 
they form two distinct classes in 
networks. 
One's at the top and that works without 
pretraining and the ones at the bottom 
and that works with pretraining. 
Each point shows a model in function 
space. 
It's no use comparing weight vectors 
because two nets might differ by having 
two of the hidden units swapped round. 
So they behave exactly the same way, but 
the weights would look very different. 
In order to compare them, you have to 
compare the functions of the 
implementation. 
And a way to do that is to have a suite 
of test cases and look at the outputs the 
networks produce on those test cases. 
And then concatenate those outputs into 
one great long vector. 
And so if two networks produce very 
similar outputs for all the test cases, 
that concatenated vector will be very 
similar for the two networks. 
Now you take those concatenated output 
vectors and you plot those in 2D using 
t-SNE. 
The colors show the stages of training so 
if you look at the networks at the top, 
there's an initial blob in dark blue. 
And then you can see that it's all moving 
roughly the same direction. 
In other words, the networks after one 
epoch of learning are all more similar to 
one another then they are to the initial 
networks. 
That's even more pronounced with the 
pre-trained networks at the bottom. 
So the color tells you which epoch you're 
in. 
The trajectories at the top without 
pre-training show that different networks 
end up in different places in function 
space. 
And they're quite widely spread. 
The trajectories of the bottom show that 
with pre-training, you end up in a quite 
different region of function space. 
And the networks tend to be more similar 
to one another. 
But the main point is there's no overlap. 
The kinds of solutions you find, if you 
pre-train the networks generatively, are 
just qualitatively different from the 
kinds of solutions you find if you start 
with small random words. 
The last thing I want to say in this 
video is to explain why pre-training 
makes sense. 
So let's imagine that the way we 
generated pairs of an image and a label 
was by taking the stuff in the real 
world, using that to generate an image, 
for example by taking a photograph of 
something. 
And then having generated the image we 
attached a label to it that didn't depend 
on the stuff in the world. 
So contingent on the image itself, the 
stuff in the world is relevant the label 
thus depends on the pixels in the image 
That would be the case for example if the 
label told us whether the top left pixel 
was similar to the bottom right pixel. 
Now if we generated images that way, then 
it would make sense to try and learn a 
mapping from images to labels. 
Because the labels depend directly on the 
images but actually, it's more plausible 
that the way we generate image label 
pairs are by there being stuff in the 
world that gives rise to the image. 
And the reason the image has the name it 
has is because of the stuff in the world, 
not because of the pixels in the image. 
So you see a cow. 
You take a photograph. 
And you call that a photograph of a cow, 
because you were looking at a cow when 
you took it. 
Now the point is, there's a high 
bandwidth from the stuff in the world to 
the image. 
And there's a low bandwidth from the 
stuff in the world to the label. 
For example if I just say cow, you don't 
know whether the cow is upside down, 
whether it was brown or black and white, 
whether it was alive or dead, how big it 
was, what else was in the image, whether 
it was facing you or facing away from 
you. 
One of those things aren't conveyed by 
the label. 
If you see an image with thousands and 
thousands of pixels, you typically know 
all of those things. 
You get much, much more information about 
the causes of an image by looking at the 
image then you do by looking at the label 
of the image. 
So in that situation, where there's a 
high bandwidth pathway from the world to 
the image, and a low bandwidth image from 
the world to the label, because the label 
typically contains very few bits. 
It makes much more sense to try and 
recover the label by first inverting the 
high boundary pathway to get back to the 
stuff in the world that caused the image. 
And then having recovered the stuff in 
the world that caused the image, to 
decide what label it would be given. 
So that's a much more plausible model of 
how we assign names to things in images. 
And that justifies having a pre-training 
phase where you try and go from the image 
to its underlying causes, followed by a 
discriminative phase where you try and go 
from those underlying causes to the 
label. 
And perhaps you slightly fine tune the 
mapping from the image to the underlying 
causes. 

