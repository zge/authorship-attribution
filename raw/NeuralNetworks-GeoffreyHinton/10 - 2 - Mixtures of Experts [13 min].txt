In this video, I'm going to talk about the
mixture of experts model that was
developed in the early 1990s.
The idea of this model is to train a
number of neural nets, each of which
specializes in a different part of the
data.
That is, we assume we have a data set
which comes from a number of different
regimes,
And we train a system in which one neural
net will specialize in each regime, and a
managing neural net will look at the input
data, and decide which specialist to give
it to.
This kind of system, doesn't make very
efficient use of data, because the data
is, fractionated over all these different
experts.
And so with small data sets, it can't be
expected to do very well.
But as data sets get bigger, this kind of
system may well come into its own, because
it can make very good use of extremely
large data sets.
In boosting, the weights on the models are
not all equal,
But after we finish training, each model
has the same weight for every test case.
We don't make the weights on the
individual models depend on which
particular case we're dealing with.
In mixture of experts, we do.
So the idea is that we can look at the
input data for a particular case during
both training and testing to help us
decide which model we can rely on.
During training this will allow models to
specialize on a subset of the cases.
They then will not learn on cases for
which they're not picked.
So they can ignore stuff they're not good
at modeling.
This will lead to individual models that
are very good at some things and very bad
at other things.
The key idea is to make each model, or
expert as we call it, focus on predicting
the right answer for cases where it's
already doing better than the other
experts.
That will cause specialization.
So there's a spectrum of models from very
local models to very global models.
Nearest neighbors, for example, is a very
local model.
To fit it, you just store the training
cases.
So, that's really simple,
And then if you have to predict Y from X,
you simply find the stored value of X
that's closest to the test value of X,
then you predict the value of Y that's the
same as for the stored value.
The result of that is that the curve
relating the input to the output consists
of lots of horizontal lines connected by
cliffs.
It would clearly make more sense to smooth
things out a bit.
At the other extreme, we have fully global
models, like fitting one polynomial to all
the data.
They're much harder to fit to data, and
they may also be unstable.
That is, small changes in the data may
cause big changes in the model you fit.
That's because each parameter depends on
all the data.
In between these two ends of the spectrum,
we have multiple local models, that are of
intermediate complexity.
This is good if the data set contains
several different regimes and those
different regimes have different
input/output relationships.
In financial data for example the state of
the economy has a big effect on
determining the mappings between inputs
and outputs, and you might want to have
different models for different states of
the economy.
But you might not know in advance how to
decide what constitutes different states
of the economy, you're going to have to
learn that too.
So we have this problem if we're going to
use different models for different
regimes, of how do we partition the data
session to these different regimes.
In order to fit different models to
different regimes we need to cluster the
training data into subsets, one for each
of these regimes.
But we don't want to cluster the data
based on the similarity of input vectors.
All we're interested in is the similarity
of input-output mappings.
So if you look at the case on the right,
there's four data points that are nicely
fitted by the red parabola and another
four data points that are nicely fitted by
the green parabola If she partition the
data based on the input I put mapping,
that is based on the idea that a parabola
will fit the data nicely, then you
partition the data where that brown line
is.
If however you partitioned the data by
just clustering the inputs, we partition
where the blue line is, and then if you
looked to the left of that blue line,
you'll be stuck with a subset of data that
can't be modeled nicely by a simple model.
So I'm going to explain an error function
that encourages models to cooperate.
And then I'm going to explain an error
function that encourages models to
specialize.
And I'm going to try to give you a good
intuition for why these two different
functions have these very different
effects.
So if you want to encourage cooperation,
what you should do is compare the average
predictors with the target and train all
the predictors together to reduce the
difference between the target and their
average.
So using angle back as for expectation
again, the error would be the difference
between the target and the average of all
the predictors of what they predict.
That will overfit badly.
It will make the model much more powerful
in training each predictor separately,
because the models will learn to fix up
the error is that other models make.
So, if you're averaging models during
training, and training so that the average
works nicely, you have to consider cases
like this.
On the right, we have the average of all
the models except for model I.
So, that's what everybody else is saying
when their votes are averaged together.
On the left, we have the output of model
I.
Now if we'd like the overall average to be
closer to the target, what do we have to
do to the output of the Ith model?
We have to move it away from the target.
That will take the overall average towards
the target.
You can see that what's happening is model
I is learning to compensate for the errors
made by all the other models.
But do we really want to move model I in
the wrong direction?
Intuitively it seems better to move model
I towards the target.
So here is an arrow function that
encourages specialization, and it's not
very different.
To encourage specialization, we compare
the output of each model with the target
separately.
We also need to use a manager to determine
the weight we put on each of these models,
which we can think of as the probability
of picking each model, if we have to pick
one.
So now, our error is the expectation over
all the different models of the squared
error made by that model times the
probability of picking that model,
Where the manager or gating network, is
determining that probability by looking at
the input for this particular case.
What will happen if you try to minimize
this error is that most of the experts
will end up ignoring most of the targets.
Each expert will only deal with the small
subset of the training cases and it will
learn to do very well on that small
subset.
So here's a picture of the mixture of
expert's architecture.
Our cost function is the squared
difference between the output of each
expert in the target averaged over all the
experts.
But with the weights in that average
determined by the manager.
It's actually a better cost function will
come to later, based on the mixture model.
But this was a cost function I first
thought of, and I think it's easier to
explain the intuition with this cost
function.
So we have an input.
Our different experts will look at that
input.
They all make their predictions based on
that input.
In addition we have a manager, a manager
might have multiple layers and the last
layer for manager is a soft max layer, so
the manager outputs as many probabilities
as there are experts,
And using the outputs of the manger and
outputs of the experts, we can then
compute the value of that error fraction.
If we look at the derivative of that other
function,
The outputs of the manager are determined
by the inputs xi to the soft max group in
the final layer of the manager.
And then the error is determined by the
outputs of the experts, and also the
probabilities output by the manager.
If we differentiate that error with
respect to the outputs of an expert, we
get a signal for training that expert and
that gradient that we get with respect to
the output of an expert is just the
probability of picking that expert, times
the difference between what that expert
says in the target.
So if the manager decides that there's a
very low probability of picking that
expert for that particular training case,
the expert will get a very small gradient,
and the parameters inside that expert
won't get disturbed by that training case.
It'll be able to save its parameters for
modeling the training cases where the
manager gives it a big probability.
We can differentiate with respect to the
outputs of the gating network.
And actually what we're gonna do is
differentiate with respect to, the
quantity that goes into the soft max.
That's called the low jet, that's xi,
And if we take the derivative with respect
to xi, we get the probability that, that
expert was picked times the difference
between the squared arrow made by that
expert and the average overall experts
when you use the weighting provided by the
manager of the squared arrow.
So what that means is, if expert I makes a
lower squared error than the average of
the other experts, then we'll try to raise
the probability of expert i.
But if expert I makes a higher squared
error than the other experts, we'll try
and lower his probability.
That's what causes specialization.
Now there's actually a better cost
function.
It's just more complicated.
It depends on mixture models, which I
haven't explained in this course.
Again, those will be well explained in
Andrew Ing's course.
I did explain, however, the interpretation
of maximum likelihood, when you're doing
regression, as the idea that the network
is actually making a Gaussian prediction.
That is the network outputs a particular
value, say Y1 and we think of it as making
bets about what the target value might be
that are a Gaussian distribution around Y1
with unit variance.
So the red expert makes a Gaussian
distribution of predictions around by Y1
and the green expert makes a prediction
around Y2.
The manager then decides probabilities for
the two experts and those probabilities
are used to scale down the Gaussians.
Those probabilities have to add to one and
they are called mixing proportions.
And so once we scale down the Gaussians we
get to distribution that's no longer a
Gaussian, is the sum of the scale down red
Gaussian and the scale down green
Gaussian.
And that's the predictive distribution
from share experts.
What we want to do now is maximize the log
probability of the target value under that
black curve and remember the black curve
is just the sum of the red curve and the
green curve.
So that leads to the following model for
the probability re-target, given a mixture
of experts.
The probability, is on the left,
And it's the sum over all the experts, of
the mixing proportion assigned to that
expert by the manager or gating network
times e squared the squared difference
between the target and the output of that
expert,
Scaled by the normalization term for a
Gaussian with a variance of one.
And so our cost function is simply going
to be the negative log of that probability
on the left.
We're going to try and minimize the
negative log of that probability.
