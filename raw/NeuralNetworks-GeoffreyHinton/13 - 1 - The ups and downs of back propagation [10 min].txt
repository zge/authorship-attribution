In this video, I'm going to talk about the
history of backpropagation.
I'll start with where it came from in the
70s' and 80s,' and then I'll talk a bit
about why it failed in the 90s.' That is,
why serious machine learning research has
abandoned it.
There was a popular view of why this
happened, and we can now see that, that
popular view was largely wrong.
The real reasons it was abandoned were
because computers were too slow and data
sets were too small.
I'll conclude by showing you a historical
document.
There was a bet made between two machine
learning researchers in 1995.
It's interesting to see what people back
then believed and how wrong they were.
Backpropagation was invented independently
several times in the 70s' and 80s'.
It started in the late 60s' with control
theories called Bryson and Ho who invented
a linear version of backpropagation.
Paul Werbos went to their lectures and
realized it could be made non-linear.
And in his thesis in 1974, he published
what's probably the first proper version
of backpropogation.
Rumelhart and Williams and I invented it
in 1981 without knowing about Paul
Werbos's work.
But we tried it out, and it didn't work
very well for the first thing we tried it
for,
And so we abandoned it.
David Parker invented it in 1985, and so
did Yann LeCun.
Also in 1985, I went back and tried again
the thing that Rumelhart, Williams and I
had abandoned and discovered it worked
pretty well.
In 1986, we produced a paper with a really
convincing example of what it could do.
It was clear the backpropagation had a lot
of promise for learning multiple layers of
non-linear future detectors.
But it didn't really live up to its
promise.
And by the late 1990s, most of the serious
researchers in machine learning had given
up on backpropagation.
For example, in David Mukai's textbook,
there's very little mention of it.
It was still widely used by psychologists
for making psychological models.
And it was also quite widely used in
practical applications, such as credit
card fraud detection.
But in machine learning, people thought it
had been supplanted by support vector
machines.
The popular explanation of what happened
to backpropagation in the late 90s' was
that it couldn't make use of multiple
layers and non-linear features.
This wasn't true in convolutional nets,
which were the exception.
But in general, people couldn't get feed
forward neural networks trained with
backpropagation to do impressive things if
they had multiple hidden layers,
Except for some toy examples.
It also did not work well in recurrent
networks or in deep auto-encoders,
Which we'll cover in our later lecture.
And recurrent networks were perhaps the
place where its most exciting, and so it
was there that it was most disappointing
that people couldn't make it work well.
Support vector machines by contrast,
worked well. They didn't require as much
expertise to make them work, they produced
repeatable results, and they had a much
better theory.
And they had much fancy of theory.
So, that was the popular explanation of
what went wrong with backpropagation.
With a more historical prospective, we can
see why really failed.
The computers were thousands of times too
slow,
And the label data's sets hundreds of
times too small for the regime in which
backpropagation would really shine.
Also, the deep networks, as well as being
too small, were not sensibly initialized.
And so, backpropagating through deep
networks didn't work well because the
gradients tended to die, because the
initial weights were typically too small.
These issues prevented backpropagation
from being successful.
For tasks like vision and speech,
They would eventually be a big win.
So, we need to distinguish between
different kinds of machine learning task.
There's ones that are more typical of the
kinds of things people study in
statistics,
And ones that are more typical of the
kinds of things people study in artificial
intelligence.
So, at the statistics end of the spectrum,
you typically have low dimensional data.
A statistician thinks of a 100 dimensions
as high dimensional data.
At the artificial intelligence end of the
spectrum, things like images or
coefficients representing speech typically
have many more than a 100 dimensions.
At the statistics end of the spectrum,
There's usually a lot of noise in the
data.
Whereas, in the AI end of the spectrum,
noise isn't the real problem.
For statistics, there's often not that
much structure in the data, and what
structure there is can be captured by a
fairly simple model.
At the AI end of the spectrum, there's
typically a huge amount of structure in
the data.
So if you take a set of images, its highly
structured data.
But the structure is too complicated to be
captured by a simple model.
So in statistics, the main problem is
separating true structure from noise, not
thinking that noise is really structure.
This can be done by a Bayesian neural net
pretty well.
But for typical non-Bayesian neural nets,
it's not the kind of problem they're good
at.
And so, for problems like that, it makes
sense to try a support vector machine or a
method called a Gaussian process if you're
doing regression, which I'll talk about
briefly later.
At the artificial intelligence end of the
spectrum, the main problem is to find a
way of representing all this complicated
structure so that it can be learned.
The obvious thing to do is to try to hand
design appropriate representations.
But actually, it's easier to let back
propagation figure out what
representations to use by giving it
multiple layers and using a lot of
computation power to let it decide what
the representation should be.
I now want to talk very briefly about
support vector machines.
I'm not going to explain how they work,
But I am going to say what I think their
limitations are.
So, there's several ways in which you can
view a support vector machine, and I'm
going to give you two different views of
them.
According to the first view, support
vector machines are just the reincarnation
of perceptions with a clever trick called
the kernel trick.
So, the idea is that you take the inputs,
You expand the raw input into a very large
lair of non-linear, but also non adaptive
features.
So, that's just like perceptrons where you
have this big layer of features it doesn't
learn.
And then, you only have to learn one layer
of adaptive weights, the weights from the
features to the decision unit.
And support vector machines have a very
clever way of avoiding overfitting when
they learn those weights.
They look for what's called a maximum
margin hyperplane in a high dimensional
space,
And they can do that much more efficiently
than you might have thought possible.
And, that's why they work well.
The second view also views support vector
machines as a clever reincarnation of
perceptrons.
But, it has a completely different notion
of what kinds of features they're using.
So, according to the second view,
Each input vector in the training set is
used to define one feature.
I'll spell it differently to indicate it's
a completely different kind of feature
from the first kind.
Each of these features gives a scale of
value which involves doing a global match
between a test input and that particular
training input.
So, roughly speaking, it's how similar the
test input is to a particular training
case.
Then, there's a clever way of
simultaneously finding how to weight those
features, so as to make the right
decision,
And also during feature selection.
That is, deciding which of those features
not to use.
So, although these views sound extremely
different from one another, they're just
two alternatives ways of looking at the
same thing,
A support vector machine.
And, in both cases,
It's using non-adaptive features and then
one layer of adaptive weights.
And that limits to what you can do that
way.
You can't learn multiple layers of
representation with a support vector
machine.
This is a historical document from 1995.
It was given to me by [unknown] and it's a
bet between Larry Jackel, who headed the
adaptive systems research group at Bell
Labs,
And Vladamir Vapnik, who is the leading
proponent of support vector machines.
Larry Jackel bet that by 2000, people
would understand why big neural nets
trained with backpropagation worked well
on large data sets.
That is, they would understand it
theoretically in terms of conditions and
bands.
Vapnik bet that they wouldn't,
But he made a side bet.
That if he was the one to figure it out,
he would win anyway.
Vapnik in turn,
Bet that by 2005, nobody will be using big
neural nets like that train
backpropagation.
It turns out that they were both wrong.
The limitation to using big neural nets
with backpropagation was not that we
didn't have a good theory and not that
they were essentially helpless, but that
we didn't have big enough computers or big
enough data sets.
It was a practical limitation not a
theoretical one.
