In this video, I'm going to return to the
idea of full Baysian learning, and explain
a little bit more about how it works.
And then in the following video, I'm going
to show how it can be made practical.
In full Bayesian learning, we don't try
and find a single best setting of the
parameters.
Instead, we try and find the full
posterior distribution over all possible
settings.
That is, for every possible setting, we
want a posterior probability density.
And all those densities, we want to add up
to one.
It's extremely computationally intensive
to compute this for all, but the simplest
models.
So, in the example earlier, we did it for
a biased coin which just has one
parameter, which is how biased it is.
But in general, for a neural net, it's
impossible.
After we've computed the posterior
distribution across all possible settings
of the parameters, we can then make
predictions by letting each different
setting of the parameters make its own
prediction.
And then, averaging all those predictions
together, weighting by their posterior
probability.
This is also very computationally
intensive.
The advantage of doing this is that if we
use the full Bayesian approach, we can use
complicated models even when we don't have
much data.
So, there's a very interesting
philosophical point here.
We're now used to the idea of overfitting,
When you fit a complicated model to a
small amount of data.
But that's basically just a result of not
bothering to get the full posterior
distribution over the parameters.
So, frequentists would say, if you don't
have much data, you should use a simple
model.
And that's true.
But it's only true if you assume that
fitting a model means finding the single
best setting of the parameters.
If you find the full posterior
distribution, that gets rid of
overfitting.
If there's very little data, the full
posterior distribution will typically give
you very vague predictions, because many
different settings of the parameters that
make very different predictions will have
significant posterior probability.
As you get more data, the posterior
probability will get more and more focused
on a few settings of the parameters, and
the posterior predictions will get much
sharper.
So, here's a classic example of
overfitting. We've got six data points and
we fitted a fifth order polynomial and so
it should go exactly through the data,
which it more or less does.
We also featured a straight line which
only has two degrees of freedom.
And so, which model do you believe?
The model that has six coefficients and
fits the data almost perfectly, or the
model that only has two coefficients and
doesn't fit the data all that well.
It's obvious that the complicated model
fits better, but you don't believe it.
It's not economical, and it also makes
silly predictions.
So, if you look at the blue arrow,
If that's the input value and you're
trying to predict the output value, the
red curve will predict a value that's
lower than any of the observed data
points, which seems crazy, whereas the
green line will predict a sense of the
value.
But everything changes, if instead of
fitting one fifth order polynomial, we
start with a reasonable prior of the fifth
order polynomials, for example, the
coefficient shouldn't be to big.
And then, we compute the full posterior
distribution over fifth order polynomials.
And I've shown you a sample from this
distribution in the picture, where a
thickened line means higher probability in
the posterior.
So, you will see some of those thin
curves, miss a few of the data points by
quite a lot, but nevertheless, they're
quite close to most of the data points.
Now, we get much vaguer, but much more
sensible predictions.
So, where the blue arrow is, you'll see
the different models predict very
different things.
While, on average, they make a prediction
quite close to the prediction made by the
green line.
From a Bayesian prospective, there's no
reason why the amount of data you collect
should influence your prior beliefs and
the complexity of the model.
A true Baysian would say, you have prior
beliefs about how complicated things might
be and just because you haven't collected
any data yet, it doesn't mean you think
things are much simpler.
So, we can approximate full Baysian
learning in a neural net, if the neural
net has very few parameters.
The idea is we put a grid over the
parameter space,
So each parameter is only allowed a few
return to values and then we take the
cross product of all those values for all
the parameters.
And now, we get a number of grid points in
the parameter space.
And in each of those points, we can see
how well our model predicts the data, that
is, if we're doing supervised learning,
how well a model predicts the target
outputs.
And we can say that the posterior
probability in that grid-point is the
product of how well it predicts the data,
how likely it is under the prior.
And with the whole thing normalized, so
that the posterior probability is
[UNKNOWN].
This is still very expensive, but notice
it has some attractive features.
There's no gradient descent involved, and
there's no local optimum issues.
We're not following a path in this space,
We're just evaluating a set of points in
this space.
Once we've decided on the posterior
probability to assign to each grid-point,
We then use them all to make predictions
on the test data.
That's also expensive.
But when there isn't much data, it'll work
much better than maximum likelihood or
maximum a posteriori.
So, the way we predict the test output,
given the test input, is we say, the
probability of the test output, given the
test input,
Is the sum overall the grid points of the
probability that, that grid-point is a
good model, is the sum over all
grid-points of the probability of that
grid-point, given the data and given our
prior, times the probability that we will
get that test output,
Given the input and given the grid-point.
In other words, we have to take into
account, the fact that we might add noise
to the output of the net before producing
the test answer.
So, here's a picture of full Bayesian
learning.
We have a little net here, that has four
weights and two biases.
If we allowed, nine possible values for
each of those weights and biases,
There would be nine to the six grid+points
in the parameter space.
It's a big number but we can cope with it.
For each of those grid-points, we compute
the probability of the observed outputs on
all the training cases.
We multiply by the prior for the
grid-point, which might depend on the
values of the weights, for example.
And then, we re-normalize to get the
posterior probability over all the
grid-points.
Then we make predictions using those
grid-points, but weight to each of their
predictions by its posterior probability.
