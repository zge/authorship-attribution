In this video, we're going to look at the 
issue of training deep autoencoders. 
People thought of these a long time ago, 
in the mid 1980s. 
But they simply couldn't train them well 
enough for them to do significantly 
better than principal components 
analysis. 
There were various papers published about 
them, but no good demonstrations of 
impressive performance. 
After we developed methods of 
pre-training deep networks one layer at a 
time. 
Russ Salakhutdinov and I applied these 
methods to pretraining deep autoencoders, 
and for the first time, we got much 
better representations out of deep 
autoencoders than we could get from 
principal components analysis. 
Deep autoencoders always seemed like a 
really nice way to do dimensionality 
reduction because it seemed like they 
should work much better than principal 
components analysis. 
They provide flexible mappings in both 
directions, and the mappings can be 
non-linear. 
Their learning time should be linear or 
better in the number of training cases. 
And after they've been learned, the 
encoding part of the network is fairly 
fast because it's just a matrix 
multiplier for each layer. 
Unfortunately, it was very difficult to 
optimize deep autoencoders using back 
propagation. 
Typically people try small initial 
weights, and then the back propagated 
gradient died, so for deep network, they 
never got off the ground. 
But now we have much better way to 
optimize them. 
We can use unsupervised layer by layer 
pre-training, 
or we can simply initialize the weight 
sensibly, as an echo statement it works. 
The first really successful deep water 
encoders were learned by Russ 
Salakhutdinov and I in 2006. 
We applied them to the N-ness digits. 
So we started with images with 784 
pixels. 
And we then encoded those via three 
hidden layers, into 30 real valued 
activities in a central code layer. 
We then decoded those 30 real valued 
activities, 
back to 784 reconstructed pixels. 
We used a stack of restricted Boltzmann 
machine to initialize the weights used 
for encoding, 
and we then took the transposers of those 
weights and initialized the decoding 
network with them. 
So initially, the 784 pixels were 
reconstructed, using a weight matrix that 
was just the transpose of the weight 
matrix used for encoding them. 
But after the four restricted Boltzmann 
machines have being trained and unrolled 
to give the transposes for decoding, 
we then applied back propagation to 
minimize the reconstruction error of the 
784 pixels. 
In this case we were using a 
cross-entropy error, because the pixels 
were represented by logistic units. 
So that error was back propagated through 
this whole deep net. 
And we once started back propagating the 
error, 
the weights used for reconstructing the 
pixels became different from the weights 
used for encoding the pixels. 
Although they, typically stayed fairly 
similar. 
This worked very well. 
So if you look at the first row, that's 
one random sample from each digit class. 
If you look at the second row, that's the 
reconstruction of the random sample by 
the deep autoencoder that uses 30 linear 
hidden units in its central layer. 
So the data has been compressed to 30 
real numbers and then reconstructed. 
If you look at the eight, you can see 
that the reconstruction is actually 
better than the eight. 
It's got rid of the little defect in the 
eight because it doesn't have the 
capacity to encode it. 
If you compare that with linear principal 
commands analysis, you can see it's much 
better. 
A linear mapping to 30 real numbers 
cannot do nearly as good a job of 
representing the data. 

