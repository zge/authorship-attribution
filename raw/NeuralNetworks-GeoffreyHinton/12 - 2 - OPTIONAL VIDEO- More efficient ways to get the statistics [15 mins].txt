In this video, I'll go into more detail
about how we can speed up the Boltzmann
Machine Learning Algorithm by using
cleverer ways of keeping Markov chains
near the equilibrium distribution, or by
using what are called mean field methods.
The material is quite advanced and so it's
not really part of the course.
There won't be any quizzes on it and it's
not on the final test.
You can safely skip this video.
It's included for people who are really
interested in how to get deep Boltzmann
machines to work well.
There are better ways of collecting the
statistics than the method that Terry
Snofsky and I originally came up with.
If we start from a random state, it may
take a long time to reach thermal
equilibrium.
Also, there's no easy tests for whether
you've reached thermal equilibrium, so we
don't know how long we need to run for.
So, the idea is why not start from
whatever state you ended up in last time
you saw that particular data vector?
So, we remember the interpretation of the
data vector in the hidden units, and we
start from there.
This stored state, the interpretation of
the data vector, is called a particle.
Using particles that persist gives us a
warm start and it has a big advantage.
If we were at equilibrium before and we
only updated the weights a little bit,
it'll only take a few updates of the units
in a particle to bring it back to
equilibrium.
We can use particles for both the positive
phase, where we have a clamp data vector,
and for the negative phase, when nothing
is clamped.
So, here's the method for directing
statistics introduced by Radford Neal in
1992.
In the positive phase, you have a set of
data specific particles, one or a few per
training case. And each particle has a
current value that's the configuration of
the hidden units plus which data vector it
goes with.
You sequentially update all the hidden
units a few times in each particle with
the relevant data vector clamped.
And then, for every connected pair of
units, you average the probability of the
two units being on over all these
particles.
In the negative phase, you keep a set of
fantasy particles.
That is, these are global configurations.
And again, after each weight update, you
sequentially update all the units in each
fantasy particle a few times.
Now, you're updating the visible units as
well.
And for every connected pair of units,
your average, SiSj, over all the fantasy
particles.
The learning rule is then the change in
the weights is proportional to the average
you got with data, averaged over all
training data, and the average you got
with the fantasy particles when nothing
was clamped.
This works better than the learning rule
that Terry Snofsky and I introduced, at
least for full batch learning.
However, it's difficult to apply this
approach to mini batches.
And the reason is, that by the time we get
back to the same data vectorn if we're
using mini batch learning, the weights
would have been updated many times.
Son the stored data specific particle for
that data vector won't be anywhere near
thermal equilibrium anymore.
The hidden units won't be in thermal
equilibrium with the visible units of the
particle given the new weights.
And again, we don't know how long we're
going to have to run for, before we get
close to equilibrium again.
So, we can overcome this by making a
strong assumption about how we understand
the world.
It's a kind of a epistemological
assumption.
We're going to assume that when a data
vector is clamped, the set of good
explanations, that is states of the hidden
units, that act as interpretations of that
data vector is uni-modal.
That means we're saying that, for a given
data vector, there aren't two very
different explanations for that data
vector.
We assume that for sensory input, there is
one correct explanation.
And if we have a good model of the data,
our model will give us one energy minimum
for that data point.
This is a restriction on the kinds of
models we're willing to learn.
We're going to use a learning algorithm
that's incapable of learning models in
which a data vector has many very
different interpretations.
Provided we're willing to make this
assumption, we can use a very efficient
method for approaching thermal equilibrium
or an approximation to thermal
equilibrium, with the data.
It's called a mean field approximation.
So, if we want to get the statistics
right, we need to update the units
statistically and sequentially.
And the update rule is the probability of
turning on unit, i, is the logistic
function of the total input it receives
from the other units in its bias.
Where Sj, the state of another unit, is a
stochastic binary thing.
Now, instead of using that rule, we could
say, we're not going to keep binary states
for unit i, we're going to keep a real
value between zero and one which we call a
probability.
And that probability at time t + one is
going to be the output of the logistic
function.
The more you put in is the bias, and the
sum of all the probabilities at time t
times the weights.
So, we're replacing this stochastic binary
thing by a real value probability.
And that's not quite right because this
stochastic binary thing is inside a
non-linear function.
If it was a linear function, things would
be fine.
But because the logistics non-linear, we
don't get the right answer when we put
probabilities instead of fluctuating
binary things inside.
However, it works pretty well.
It can go wrong by giving us biphasic
oscillations because now we're going to be
updating everything in parallel.
And we can normally deal with those by
using what's called damped mean field
where we compute that pi of t1.
+ one.
But, we don't go all the way there.
We go to a point in between where we are
now, and where this update wants us to go.
So, in damped mean field, we'll go to
lambda times the place we are now, plus
one minus lambda times the place the
update rule tells us to go to.
And that will kill oscillations.
Now, we can get an efficient mini batch
learning procedure for both the machines,
and this is what Russ Salakhutdinov
realized.
In the positive phase, we can initialize
all probabilities at 0.5.
We can clamp a data vector on the visible
units, and we can update all the hidden
units in parallel using mean field until
convergence.
And for mean field, you can recognize
convergence is when the probability stop
changing.
And once we converged, we record PiPj for
every connected pair of units.
In the negative phase, we do what we were
doing before.
We keep a set of fantasy particles, each
of which has a value that's a global
configuration.
And after each weight update, we
sequentially update all the units in each
fantasy particles a few times.
And then, for every connected pair of
units, we average SiSj, these stochastic
binary things, over all fantasy particles.
And the difference in those averages is
the learning rule.
That is, we change the weights by an
amount proportional to that difference.
If we want to make the updates for the
fantasy particles more parallel, we can
change the architecture of the Boltzmann
machine.
So, we're going to have a special
architecture that allows alternating
parallel updates for the fantasy
particles.
We have no connections within a layer, and
we have no skip-layer connections, but we
allow ourselves lots of hidden layers.
So, the architecture looks like this.
We call it a Deep Boltzmann Machine.
And, it's really a general Boltzmann
machine with lots of missing connections.
All those skipped layer connections, if
they were present.
We wouldn't really have layers at all, it
would just be a general Boltzmann machine.
But, in this special architecture, there's
something nice we can do.
We can update the states for example the
first hidden layer and the third hidden
layer, given the current states of the
visible units and the second hidden layer.
And then, we can update the states of the
visible units in the second hidden layer.
And then, we can go back and update the
other states,
And we can go backwards and forwards like
this.
And so, we can update half the states of
all the units in parallel and that'll be a
correct update.
So, one question is, if we have a deep
Boltzmann machine like that trained by
using mean field for the positive phase
and updating fantasy particles by
alternating between even layers and odd
layers for the negative phase, can we
learn good models of things like the MNIST
digits, or indeed, a more complicated
things?
So, one way to tell whether you've learned
a good model is after learning, you remove
all the input and you just generate
samples from your model.
So, you run the Markov chain for a long
time until it's burned in, and then you
look at the samples you get.
So, Russ Salakhutdinov used a eep
Boltzmann machine to model MNIST digits
using mean field for the positive phase,
And alternating updates of the layers of
the particles for the negative phase.
And the real data looks like this.
And the data that he got from his model
looks like this.
You can see, they're actually fairly
similar.
The model is producing things very like
the MNIST digits so it's learned a pretty
good model.
So here's a puzzle.
When he was learning that, he was using
mini-batches with 100 data examples and
also he was using 100 fantasy particles,
the same 100 fantasy particles for every
mini-batch.
And the puzzle is, how can we estimate the
negative statistics with only 100 negative
examples to characterize the whole space?
For all interesting problems, the global
configurations base is going to be highly
multi model.
And how do we manage to find and represent
all the nodes with only 100 particles?
There's an interesting answer to this.
The learning interacts with the Markov
chain that's being used to gather the
negative statistics, either one that's
used to update the fantasy particles, and
it interacts with it to make it have a
much higher effective mixing rate.
That means, we cannot analyze the learning
by thinking of it being an outer loop that
updates the weights,
And an inner loop that gathers statistics
with a fixed set of weights.
The learning is affecting how effective
that inner loop is.
The reason for this is that whenever the
fantasy particles outnumber the positive
data, the energy surface is raised, and
this has an effect on the mixing rate of
the Markov chain.
It makes the fantasies rush around
hyper-actively,
And they move around much faster than the
mixing rate of the mark of chain to find
better current static weights.
So, here's a picture that shows you what's
going on.
If there's a mode in the energy surface
that has more fantasy particles than data,
the energy surface will be raised until
the fantasy particles escape from that
mode.
So, the mode on the left has four fantasy
particles and only two data points. So,
the effect of the learning is going to be
to raise the energy there.
And that energy barrier might be much too
high for a Markov chain to be able to
cross, so the mixing rate will be very
slow.
But, the learning will actually spill
those red particles out of that energy
minimum by raising the minimum.
And we get filled up and the fantasy
particles will escape and go off somewhere
else, to some other deep minimum.
So, we can get out of minima that the
Markov chain would not be able to get out
of, at least, not in a reasonable time.
So, what's going on here is the energy
surface is really being used for two
different purposes.
The energy surface represents our model,
but it's also being manipulated by the
learning algorithm to make the Markov
chain mix faster.
Or rather, to have the effect of a
faster-mixing Markov chain.
Once the fantasy particles have filled up
one hole, they'll rush off to somewhere
else and deal with the next problem.
An analogy for them is that their like
investigative journalists who rush in to
investigate some nasty problem.
As soon as the publicity has caused that
problem to be fixed, instead of saying,
okay, everything is okay now.
They rush off to find the next nasty
problem.
