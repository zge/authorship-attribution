In this lecture, I'll introduce belief
nets.
One of the reason I abandoned back
propagation in the 1990's is because it
required too many labels.
Back then, we just didn't have data sets
with sufficient numbers of labels.
I was also influenced by the fact that
people managed to learn with very few
explicit labels.
However, I didn't want to abandon the
advantages of doing gradient descend
learning to learn a whole bunch of
weights.
So the issue was, was there another
objective function that we could do
grading decentive?
The obvious place to look was generative
models where the objective function is to
model the input data rather than
predicting a label.
This meshed nicely with a major movement
in statistics and artificial intelligence
called the graphical models.
The idea of graphical models was to
combine discrete graph structures for
representing how variables depended on one
another.
With real valued computations that
inferred the probability of one variable,
given the observed values of other
variables.
Boltzmann Machines were actually a very
early example of a graphical model,
But they were undirected graphical models.
In 1992, Radford Neal pointed out that
using the same kinds of units as we used
in Boltzmann machines, we could make
directed graphical models which he called
Sigmoid Belief Nets.
And the issue then became, how can we
learn Sigmoid belief nets?
The second problem is that for deep
networks, the learning time does not scale
well.
When there's multiple hidden layers, the
learning was very slow.
You might ask why this was,
And we now know that one of the reasons
was we did not initialize the weights in a
sensible way.
Yet, another problem is the back
propagation can get stuck in poor local
optima.
These are often quite good, so back
propagation is useful.
But we can now show that for deep nets,
the local optima you get stuck in, if you
start with small random weights are
typically far from optimal.
There is the possibility of retreating to
simpler models that allow convex
optimization.
But, I don't think this is a good idea.
Mathematicians like to do that because
they can prove things.
But in practice, you're just running away
from the complexity of real data.
So, one way to overcome the limits of back
propagation is by using unsupervised
learning.
The idea is that we want to keep the
efficiency and simplicity of using a
gradient method and stochastic mini batch
descent for adjusting weights.
But, we're going to use that method for
modeling the structure of the sensory
input, not for modeling the relation
between input and output.
So the idea is, the weights are going to
be adjusted to maximize the probability
that a generative model would have
generated the sensory input.
We already saw that in learning Boltzmann
machines.
And one way to think about it is, if you
want to do computer vision, you should
first learn to do computer graphics.
To first order, computer graphics works
and computer vision doesn't.
The learning objective for a generative
model, as we saw with Boltzmann machines,
is to maximize the probability of the
observed data not to maximize the
probability of labels given inputs.
Then the question arises, what kind of
generative model should we learn?
We might learn an energy based model like
the Boltzmann machine,
Or we might learn a causal model made of
idealized neurons, and that's what we'll
look at first.
Well finally, we might learn some kind of
hybrid of the two, and that's where we'll
end up.
So, before I go into causal belief nets
made of neurons, I want to give you a
little bit of background about artificial
intelligence and probability.
In the 1970's and early 1980's, people in
artificial intelligence were unbelievably
anti-probability.
When I was a graduate student, if you
mentioned probability, it was assigned
that you were stupid and that you just
hadn't got it.
Computers were all about discrete single
processing, and if you'd introduce any
probabilities they would just infect
everything.
It's hard to conceive of how much people
are against probability, so here's a quote
to help you.
I'll read it out.
Many ancient Greeks supported Socrates
opinion that deep, inexplicable thoughts
came from the gods.
Today's equivelant to those gods is the
erratic, even probabilistic neuron.
It is more likely that increased
randomness of neural behavior is the
problem of the epileptic and the drunk,
not the advantage of the brilliant.
That was in Patrick Henry Winston's first
AI textbook, in the first edition.
And it was the general opinion at the
time.
Winston was to become the leader of the
MIT AI Lab.
Here's an alternative view.
All of this will lead to theories of
computation which are much less rigidly of
an all-or-none nature than past and
present formal logic.
There are numerous indications to make us
believe that this new system of formal
logic will move closer to another
discipline which has been little linked in
the past with logic.
This is thermodynamics, primarily in the
form it was received from Boltzmann.
That was written by John von Neumann in
1957, and was part of the unfinished
manuscript he left behind for what was to
be his crowning achievement,
His book on The Computer and the Brain.
I think if von Neumann had lived, the
history of artificial intelligence might
have been somewhat different.
So, probabilities eventually found their
way into AI by something called graphical
models,
Which are a marriage of graph theory and
probability theory.
In the 1980's, there was a lot of work on
expert systems in AI that use bags of
rules for tasks such as, medical diagnosis
or exploring for minerals.
Now, these were practical problems so they
had to deal with uncertainty.
They couldn't just use toy examples where
everything was certain.
People in AI dislike probability so much
that even when they were dealing with
uncertainty, they didn't want to use
probabilities.
So, they made up their own ways of dealing
with uncertainties that did not involve
probabilities.
You can actually prove that this is a bad
bet.
Graphical models were introduced by Pearl,
Heckman, Lauritz and many others who
shared that probabilities actually worked
better than the ad hoc methods developed
by people doing expert systems.
Discrete graphs were good for representing
what variable dependent on what other
variables.
But once you have those graphs, you then
needed to do real value computations that
respected the rules of probability so that
you could compute the expected values of
some nodes in the graph, given the
observed states of other nodes.
Belief nets is the name that people in
graphical models give to a particular
subset of graphs which are directed
acyclic graphs.
And typically, they use sparsely connected
ones.
And if those graphs are sparsely
connected, they have clever inference
algorithms that can compute the
probabilities of unobserved nodes
efficiently.
But, these clever of algorithms are
exponential in the number of nodes that
influence each node, so they won't work
for densely connected nodes.
So, belief net is directed acyclic graph
composed of stochastic variables,
And here's a picture of one.
In general, you might observe any of the
variables.
I'm going to restrict myself to nets in
which you only observe the leaf nodes.
So, we imagine is these unobserved hidden
causes, and they may be lead,
And they eventually give rise to some
observed effects.
Once we observe some variables, there's
two problems we'd like to solve.
The first is what I call the inference
problem, and that's to infer the states of
unobserved variables.
Of course, we can't infer them with
certainty, so what we're after is the
probability distributions of unobserved
variables.
And if unobserved variables are not
independent of one another, given the
observed variables, there is probability
distributions are likely to be big
cumbersome things with an exponential
number of terms in.
The second problem is the learning
problem.
That is, given a training set composed of
observed vectors of states of all of the
leaf nodes,
How do we adjust the interactions between
variables to make the network more likely
to generate that training data?
So, adjusting the interactions would
involve both deciding which node is
affected by which other node,
And also deciding on the strength of that
effect.
So, let me just say a little bit about the
relationship between graphical models and
neural networks.
The early graphical models used experts to
define the graph structure and also the
conditional probabilities.
They would typically take a medical expert
and ask him how likely is this to cause
that, and then they would make a graph in
which the nodes had meanings.
And they typically had conditional
probability tables that described how a
set of values for the parents of a node
would determine the distribution of values
for the node.
Their graph was sparsely connected.
And the initial problem they focused on
was how to do correct inference.
Initially, they weren't interested in
learning because the knowledge came from
the experts.
By contrast, for neural nets, learning was
always a central issue and hand wiring the
knowledge was regarded as not cool.
Although, of course, wiring in some basic
properties, as in convolutional nets, was
a very sensible thing to do.
But basically, the knowledge in the net
came from learning the training data, not
from experts.
Neural networks didn't aim to have
interpretability or sparse connectivity to
make the inference easy.
Nevertheless, there are neural network
versions of belief nets.
So, if we think about how to make
generative models out of idealized
neurons,
There's basically two types of generative
model you can make.
This energy based models, where you
connect binary stochastic neurons using
symmetric connections, and then you get a
Boltzmann machine.
A Boltzmann machine, as we've seen, is
hard to learn.
But if we restrict the connectivity, then
it's easy to learn a restricted Boltzmann
machine.
However, when we do that, we've only
learned one hidden layer.
And so, we're giving up on a lot of the
power of neural nets with multiple hidden
layers in order to make learning easy.
The other kind of model you can make is a
causal model.
That is a directed acyclic graph composed
of binary stochastic neurons.
And when you do that, you get a sigmoid
belief net.
In 1992, Neal introduced models like this
and compared them with Boltzmann machines
and showed that Sigmoid belief nets were
slightly easier to learn.
So, a Sigmoid belief net is just a belief
net in which all of the variables are
binary stochastic neurons.
To generate data from this model, you take
the neurons in the top layer.
You determine whether they should be ones
or zeros based on their biases,
So you determine out stochastically.
And then, given the states of the neurons
in the top layer, you'd make stochastic
decisions about what the neurons in the
middle layer should be doing.
And then, given their binary states, you
make decisions about what the visible
effect should be.
And by doing that sequence of operations,
a causal sequence from layer to layer,
You would get an unbiased sample of the
kinds of vectors of visible values that
your neural network believes in.
So, in a causal model, unlike a Boltzmann
machine, it's easy to generate samples.
