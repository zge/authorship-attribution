In this video, I'm going to introduce 
Principal Component Analysis, which is a 
very widely used technique in signal 
processing. 
The idea of principal components 
analysis, is that high dimensional data 
can often be represented using a much 
lower dimensional code. 
This happens when the data lies near a 
linear manifold in the high dimensional 
space. 
So the idea is, if we can find this 
linear manifold, we can project the data 
onto the manifold, 
and then just represent where it is on 
the manifold. 
And we haven't lost much, 
because in the directions orthogonal to 
the manifold, there's not much variation 
in the data. 
As we'll see, we can do this operation 
efficiently using standard principle 
components methods, 
or we can do it inefficiently using a 
neural net with one hidden layer, where 
both the hidden units and the output 
units are linear. 
The advantage of doing it with the neural 
net is that we can then generalize the 
technique to using deep neural nets in 
which the code is a nonlinear function of 
the input. 
And our reconstruction of the data from 
the code is also a nonlinear function of 
the code. 
This enables us to deal with curved 
manifolds in the input space. 
So we represent data by where it gets 
projected on the curve manifold. 
And this is a much more powerful 
representation. 
In principal components analysis, we have 
N dimensional data, and we want to 
represent it using less than N numbers. 
And so we find M orthagonal directions in 
which the data has the most variance. 
And we ignore the directions in which the 
data doesn't vary much. 
The M principal dimensions form a lower 
dimensional subspace, and we represent an 
N dimensional data point by its 
projections onto these M dimensions in 
the lower dimensional space. 
So we've lost all information about where 
the data point is located in the 
remaining orthogonal directions. 
But since these don't have much variance, 
we haven't lost that much information. 
If we wanted to reconstruct the data 
point, from our representation in terms 
of M numbers, we reduce the mean value 
for all the N minus M directions that are 
not represented, and then the area in our 
reconstruction. 
Would be the sum over all these 
unrepresented directions of the square 
difference between the value of the data 
point count on that direction, and the 
mean value on that direction. 
This is most easily seen in the picture. 
So consider 2-dimensional data. 
This distributed according to an 
elongated Gaussian like this. 
The ellipse is meant to show kind of one 
standard deviation contour of the 
Gaussian. 
And consider a data point like that red 
one. 
If we used principal components analysis 
with a single component. 
That component would be the direction in 
the data that had the greatest variance. 
And so to represent the red point, we'd 
represent how far along that direction it 
lay. 
In other words we'd represent the 
projection of the red point onto that 
line, i.e. 
the green point. 
When we need to reconstruct the red 
point, 
what we'll do is simply use the mean 
value of all the data points, in the 
direction that we've ignored. 
In other words you'll represent a point 
on that black line. 
And so the lost in the reconstruction 
will be the squared difference between 
the red point and the green point. 
That is, would have lost the difference 
between the data point and the mean value 
of all the data, in the direction we're 
not representing, which is the direction 
of least variance. 
And so we obviously have minimized our 
loss if we choose to ignore the direction 
of least variance. 
Now, we can actually implement PCA, or a 
version of it, using back propagation, 
but it's not very efficient. 
So what we do is we make a network in 
which the output of the network is the 
reconstruction of the data. 
And we try and minimize the squared error 
in the reconstruction. 
The network has a central bottleneck that 
only has M hidden units. 
And those are going to correspond to the 
principal components, or something like 
them. 
So it looks like this. 
We have an input vector. 
We project that onto a code vector. 
And from the code vector, we construct an 
output vector. 
And the aim is to make the output vector 
as similar as possible to the input 
vector. 
The activities of the hidden units in the 
code vector from a bottleneck. 
So the code vector is a compressed 
representation of the input vector. 
If the hidden units and the output units 
are linear, then ordering coder like 
this, 
we'll learn codes that minimize the 
squared reconstruction error. 
And that's exactly what principle 
components analysis does. 
It will get exactly the same 
reconstruction error as principle 
components analysis does. 
But it won't necessarily have hidden 
units that correspond exactly the, the 
principle components. 
They will span the same space as the 
first N-principal components, but there 
may be a rotation and skewing of those 
axes. 
So the incoming weight vectors of the 
code units, which are what represent the 
directions of the components, may not be 
orthogonal. 
And unlike principal components analysis, 
they will typically have equal variances. 
But the space spanned by the incoming 
weight vectors of those code units will 
be exactly the same as the space spanned 
by the M principal components. 
So in that sense this network will do an 
equivalent thing to principal components. 
It's just if we use to cast the great 
descend learning for this network, it 
will typically much less sufficient than 
the algorithm used for principle 
components. 
Although if there's a huge amount of 
data, it might actually be more 
efficient. 
The main point of implementing principal 
component analysis using back-propagation 
in a neural net is that it allows us to 
generalize principal component analysis. 
If we use a neural net that has nonlinear 
layers before and after the code layer, 
it should be possible to represent data 
that lies on a curved manifold rather 
than a linear manifold in a 
high-dimensional space. 
And this is much more general. 
So our network will look something like 
this, the B input vector, and then one or 
more, layer of non-linear hidden unit, 
typically we use logistic units. 
Then there'll be a code layer which might 
be linear units. 
And then following the code layer, 
there'll be one or more layers of 
non-linear hidden units. 
And then there'll be an output vector, 
which we train to be as similar as 
possible to the input vector. 
So this is a curious network in which 
we're using a supervised learning 
algorithm to do unsupervised learning. 
The bottom part of the network is an 
encoder. 
Which takes the input vector and convert 
it into a code using a non-linear method. 
The top part of the network is a decoder, 
which takes the nonlinear code and maps 
it back to a reconstruction of the input 
vector. 
So after we've done the learning, we have 
mappings in both directions. 

