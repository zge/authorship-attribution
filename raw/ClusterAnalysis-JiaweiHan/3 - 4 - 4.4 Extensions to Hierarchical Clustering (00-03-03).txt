[MUSIC]
Hi, in this and subsequent sessions,
we are going to discuss several
Extensions to Hierarchical Clustering.
Hierarchical clustering methods
look simple, but on the other hand,
there are some weaknesses of
hierarchical clustering method.
The first weakness is the master can
never undo what was done previously.
For dividing methods, you first try
to figure out how to divide one
cluster into two subclusters.
But once you divide them
into two subclusters,
you work on each subcluster and
try to do further splitting.
You will never be able to
merge them back again and
try to adjust to some particular elements.
Even for agglomerative clustering,
the philosophy is similar in a sense.
Once you try to merge two
clusters into one, the subsequent
you know, analysis will be
treating this one as one unit.
You will never split it again, and
you try to see whether other subcluster
obtained so far can be further merged.
This, you know, require you for
every split or merge must be final.
That's too high requirement to
generate high-quality clusters.
The second problem is the masters
may not scale well just because
every time when you try to merge,
you try to check all the possible pairs.
So the complexity is at least n square,
when you want to split,
you try many different possible choices,
to try to find the best to split but
complexity is also high.
There are some developments other
hierarchical clustering algorithms.
In this lecture,
we are going to introduce three of them.
One is a BIRCH, developed in 1996,
you use micro-clustering,
macro-clustering idea, so
using clustering feature tree and
incrementally adjust
the quality of subclusters.
The second one we are going to introduce
is called CURE, developed in 1998.
That method essentially is to,
representing the cluster using a set of
well-scattered representative points.
The third one, CHAMELEON,
it was developed in 1999,
it used graph partitioning methods on
K-nearest neighbor graph of the data.
We will introduce all these three,
one by one.
[MUSIC]

